{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deeper2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "R3jTUl3fWasq",
        "OfMYVo0iQ3DA",
        "8jMStDBKXQ-m",
        "osvzwrKYXJR2",
        "1PvMtViAXS0V",
        "jdP40E-BXcXI",
        "OFmxBSIUXfWh",
        "77NDYbXHXn77",
        "wyW4qmuIX7cp"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ibacaraujo/deeper/blob/master/deeper2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDx6VTDuWW_2",
        "colab_type": "text"
      },
      "source": [
        "# DeepER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3jTUl3fWasq",
        "colab_type": "text"
      },
      "source": [
        "### Install packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pt-KqqidAVi3",
        "colab_type": "code",
        "outputId": "a6fb4c7f-fc82-43e1-a693-e71bb754e4b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "%%shell\n",
        "pip uninstall scikit-learn\n",
        "pip install scikit-learn==0.21"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling scikit-learn-0.22.2.post1:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.6/dist-packages/scikit_learn-0.22.2.post1.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/sklearn/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Collecting scikit-learn==0.21\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b7/6c/ec121123c671d980c6969dfc69d0f09e1d7f88d80d373f511e61d773b85c/scikit_learn-0.21.0-cp36-cp36m-manylinux1_x86_64.whl (6.6MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6MB 1.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.21) (0.14.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.21) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.21) (1.18.2)\n",
            "Installing collected packages: scikit-learn\n",
            "Successfully installed scikit-learn-0.21.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9q7jjyowCyF4",
        "colab_type": "code",
        "outputId": "aa31cb8d-011f-49e3-f1ce-ea1a6b509a0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615
        }
      },
      "source": [
        "%%shell\n",
        "pip install py-entitymatching"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: py-entitymatching in /usr/local/lib/python3.6/dist-packages (0.3.2)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.6/dist-packages (from py-entitymatching) (2.4.6)\n",
            "Requirement already satisfied: cloudpickle>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from py-entitymatching) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from py-entitymatching) (1.4.1)\n",
            "Requirement already satisfied: ipython==5.6 in /usr/local/lib/python3.6/dist-packages (from py-entitymatching) (5.6.0)\n",
            "Requirement already satisfied: numpy==1.16.2 in /usr/local/lib/python3.6/dist-packages (from py-entitymatching) (1.16.2)\n",
            "Requirement already satisfied: PyPrind in /usr/local/lib/python3.6/dist-packages (from py-entitymatching) (2.11.2)\n",
            "Requirement already satisfied: py-stringsimjoin>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from py-entitymatching) (0.3.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.6/dist-packages (from py-entitymatching) (0.21.0)\n",
            "Requirement already satisfied: matplotlib>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from py-entitymatching) (3.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from py-entitymatching) (2.21.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython==5.6->py-entitymatching) (0.8.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython==5.6->py-entitymatching) (46.1.3)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython==5.6->py-entitymatching) (1.0.18)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython==5.6->py-entitymatching) (4.3.3)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython==5.6->py-entitymatching) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython==5.6->py-entitymatching) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython==5.6->py-entitymatching) (0.7.5)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython==5.6->py-entitymatching) (2.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from py-stringsimjoin>=0.3.0->py-entitymatching) (0.14.1)\n",
            "Requirement already satisfied: pandas>=0.16.0 in /usr/local/lib/python3.6/dist-packages (from py-stringsimjoin>=0.3.0->py-entitymatching) (1.0.3)\n",
            "Requirement already satisfied: py-stringmatching>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from py-stringsimjoin>=0.3.0->py-entitymatching) (0.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from py-stringsimjoin>=0.3.0->py-entitymatching) (1.12.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.4->py-entitymatching) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.4->py-entitymatching) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.4->py-entitymatching) (1.2.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->py-entitymatching) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->py-entitymatching) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->py-entitymatching) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->py-entitymatching) (3.0.4)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython==5.6->py-entitymatching) (0.1.9)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython==5.6->py-entitymatching) (0.2.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython==5.6->py-entitymatching) (0.6.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.16.0->py-stringsimjoin>=0.3.0->py-entitymatching) (2018.9)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ky1_WievbO4G",
        "colab_type": "code",
        "outputId": "eef88115-70ea-49b2-b99d-2e272637ae30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "%%shell\n",
        "\n",
        "pip install fasttext"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fasttext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/61/2e01f1397ec533756c1d893c22d9d5ed3fce3a6e4af1976e0d86bb13ea97/fasttext-0.9.1.tar.gz (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 1.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.5.0)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (46.1.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.16.2)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.1-cp36-cp36m-linux_x86_64.whl size=2387325 sha256=f6194c942a137c343c30d63ecb78ecb9f9732ce244876cd0af532a62e0937509\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f0/04/caa82c912aee89ce76358ff954f3f0729b7577c8ff23a292e3\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.9.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPDMMCSGiZFE",
        "colab_type": "code",
        "outputId": "e9da119e-c35e-46d2-b758-e7cdb36f4e3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "%%shell\n",
        "\n",
        "pip install torchfile"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchfile\n",
            "  Downloading https://files.pythonhosted.org/packages/91/af/5b305f86f2d218091af657ddb53f984ecbd9518ca9fe8ef4103a007252c9/torchfile-0.1.0.tar.gz\n",
            "Building wheels for collected packages: torchfile\n",
            "  Building wheel for torchfile (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchfile: filename=torchfile-0.1.0-cp36-none-any.whl size=5712 sha256=c77956c46452f051765c5bfa33bfbe90bb810c03f175faad5907a1324f52600d\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/c3/d6/9a1cc8f3a99a0fc1124cae20153f36af59a6e683daca0a0814\n",
            "Successfully built torchfile\n",
            "Installing collected packages: torchfile\n",
            "Successfully installed torchfile-0.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfMYVo0iQ3DA",
        "colab_type": "text"
      },
      "source": [
        "### Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PA1Ptk3zQ6tw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "4a393a05-0679-4840-de1c-b3e6b2bd0cc9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mE8Ag15RaeI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "99e3dbcb-7a98-472e-a206-ee63f5dcaf95"
      },
      "source": [
        "%%shell\n",
        "ls \"/content/drive/My Drive\""
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Colab Notebooks'   data-science-consultancy\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jMStDBKXQ-m",
        "colab_type": "text"
      },
      "source": [
        "### Configs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycL2uSgoXSeP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATASET_ROOT = \"/content/drive/My Drive/data-science-consultancy/BenchmarkDatasets/\"\n",
        "#Note: This contains also the label data!\n",
        "CANDSET_IDS_FILE_NAME = \"candset_ids_only.csv\"\n",
        "\n",
        "#TODO: Change me\n",
        "CANDSET_FILE_NAME = \"candset.pkl.compress\"\n",
        "\n",
        "#Size of distributed representations\n",
        "#DR_DIMENSION = 300\n",
        "FASTTEXT_MODEL_PATH = \"/content/drive/My Drive/data-science-consultancy/fasttext/cc.pt.300.bin\"\n",
        "\n",
        "#Assumptions: partially for interoperability with Magellan/py_entitymatching\n",
        "# 1. id column is always named as \"id\"\n",
        "# 2. all relevant attributes have the same name. any columns without same names will be ignored\n",
        "\n",
        "er_dataset_details = {\n",
        "        \"Abt_Buy\" : {\n",
        "            \"dataset_folder_path\" : DATASET_ROOT + \"Abt_Buy/\",\n",
        "            \"ltable_file_name\" : \"Abt.csv\",\n",
        "            \"rtable_file_name\" : \"Buy.csv\",\n",
        "            \"golden_label_file_name\" : \"abt_buy_perfectMapping.csv\",\n",
        "        },\n",
        "\n",
        "        \"Amazon_GoogleProducts\" : {\n",
        "            \"dataset_folder_path\" : DATASET_ROOT + \"Amazon_GoogleProducts/\",\n",
        "            \"ltable_file_name\" : \"Amazon.csv\",\n",
        "            \"rtable_file_name\" : \"GoogleProducts.csv\",\n",
        "            \"golden_label_file_name\" : \"Amzon_GoogleProducts_perfectMapping.csv\",\n",
        "        },\n",
        "\n",
        "        \"Walmart_Amazon\" : {\n",
        "            \"dataset_folder_path\" : DATASET_ROOT + \"Walmart_Amazon/\",\n",
        "            \"ltable_file_name\" : \"walmart.csv\",\n",
        "            \"rtable_file_name\" : \"amazon.csv\",\n",
        "            \"golden_label_file_name\" : \"matches_walmart_amazon.csv\",\n",
        "        },\n",
        "\n",
        "        \"DBLP_ACM\" : {\n",
        "            \"dataset_folder_path\" : DATASET_ROOT + \"DBLP_ACM/\",\n",
        "            \"ltable_file_name\" : \"DBLP2.csv\",\n",
        "            \"rtable_file_name\" : \"ACM.csv\",\n",
        "            \"golden_label_file_name\" : \"DBLP-ACM_perfectMapping.csv\",\n",
        "        },\n",
        "\n",
        "        \"DBLP_Scholar\" : {\n",
        "            \"dataset_folder_path\" : DATASET_ROOT + \"DBLP_Scholar/\",\n",
        "            \"ltable_file_name\" : \"DBLP1.csv\",\n",
        "            \"rtable_file_name\" : \"Scholar.csv\",\n",
        "            \"golden_label_file_name\" : \"DBLP-Scholar_perfectMapping.csv\",\n",
        "        },\n",
        "\n",
        "        \"Cora\" : {\n",
        "            \"dataset_folder_path\" : DATASET_ROOT + \"Cora/\",\n",
        "            \"ltable_file_name\" : \"cora.csv\",\n",
        "            \"rtable_file_name\" : \"cora.csv\",\n",
        "            \"golden_label_file_name\" : \"matches_cora.csv\",\n",
        "        },\n",
        "\n",
        "        \"Fodors_Zagat\" : {\n",
        "            \"dataset_folder_path\" : DATASET_ROOT + \"Fodors_Zagat/\",\n",
        "            \"ltable_file_name\" : \"fodors.csv\",\n",
        "            \"rtable_file_name\" : \"zagats.csv\",\n",
        "            \"golden_label_file_name\" : \"matches_fodors_zagats.csv\",\n",
        "        },\n",
        "\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osvzwrKYXJR2",
        "colab_type": "text"
      },
      "source": [
        "### Blocking utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z139_LZDXPrU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import py_entitymatching as em\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def load_known_dataset(dataset_name):\n",
        "    dataset_dtls = er_dataset_details[dataset_name]\n",
        "    l_file_name = dataset_dtls[\"dataset_folder_path\"] + dataset_dtls[\"ltable_file_name\"]\n",
        "    r_file_name = dataset_dtls[\"dataset_folder_path\"] + dataset_dtls[\"rtable_file_name\"]\n",
        "\n",
        "    #Assumption: key name is always \"id\"\n",
        "    A = em.read_csv_metadata(l_file_name , key=\"id\", encoding='utf-8')\n",
        "    B = em.read_csv_metadata(r_file_name , key=\"id\", encoding='utf-8')\n",
        "\n",
        "    return A, B\n",
        "\n",
        "#Given a dataset name, it returns all duplicates as a Pandas DF\n",
        "#Assumes the first row is a header\n",
        "def get_all_duplicates_as_df(dataset_name):\n",
        "    dataset_dtls = er_dataset_details[dataset_name]\n",
        "    duplicates_file_name = dataset_dtls[\"dataset_folder_path\"] + dataset_dtls[\"golden_label_file_name\"]\n",
        "    duplicates_df = pd.read_csv(duplicates_file_name)\n",
        "    return duplicates_df\n",
        "\n",
        "#Given a dataset name and candidate set df, this adds a column called gold.\n",
        "# it is set to  1 for all duplicates and 0 for others\n",
        "def add_labels_to_candset(dataset_name, candset_df, objectify=False):\n",
        "    duplicates_df = get_all_duplicates_as_df(dataset_name)\n",
        "    #rename columns to make merging easier\n",
        "    duplicates_df.columns = [\"ltable_id\", \"rtable_id\"]\n",
        "\n",
        "    #Sometimes pandas / Magellan puts some columns as objects instead of numeric/string. In this case, we will force this to join appropriately\n",
        "    if objectify:\n",
        "        duplicates_df = duplicates_df.astype(object)\n",
        "\n",
        "    #We merged two DF based on the common attributes. The indicator 'gold' takes three values both, left_only, right_only\n",
        "    df_with_gold = pd.merge(candset_df, duplicates_df, on=['ltable_id', 'rtable_id'], how='left', indicator='gold')\n",
        "    #If it is present in both, then it is a duplicate and we set it to 1 and 0 otherwise\n",
        "    df_with_gold['gold'] = np.where(df_with_gold.gold == 'both', 1, 0)\n",
        "\n",
        "    #print df_with_gold.loc[df_with_gold['gold']==1,['ltable_id', 'rtable_id']]\n",
        "    return df_with_gold\n",
        "\n",
        "def save_candset_ids_only(dataset_name, candset_df):\n",
        "    dataset_dtls = er_dataset_details[dataset_name]\n",
        "    output_file_name = dataset_dtls[\"dataset_folder_path\"] + CANDSET_IDS_FILE_NAME\n",
        "    candset_df[ ['ltable_id', 'rtable_id', 'gold'] ].to_csv(output_file_name, index=False)\n",
        "\n",
        "def save_candset_compressed(dataset_name, candset_df, file_name):\n",
        "    dataset_dtls = er_dataset_details[dataset_name]\n",
        "    output_file_name = dataset_dtls[\"dataset_folder_path\"] + file_name\n",
        "    candset_df.to_pickle(output_file_name, compression=\"gzip\")\n",
        "\n",
        "#This function takes the dataset name and block_fn is a function pointer for each dataset used for its blocking\n",
        "# This stores both the candset in a compressed file and also the ids of candset\n",
        "def save_candset_wrapper(dataset_name, block_fn, objectify=False):\n",
        "    A, B = load_known_dataset(dataset_name)\n",
        "    candset_df = block_fn(A, B)\n",
        "    candset_with_labels_df = add_labels_to_candset(dataset_name, candset_df, objectify)\n",
        "    save_candset_compressed(dataset_name, candset_with_labels_df, CANDSET_FILE_NAME)\n",
        "    save_candset_ids_only(dataset_name, candset_with_labels_df)\n",
        "\n",
        "def load_candset_compressed(dataset_name, file_name):\n",
        "    dataset_dtls = er_dataset_details[dataset_name]\n",
        "    input_file_name = dataset_dtls[\"dataset_folder_path\"] + file_name\n",
        "\n",
        "    #Magellan complains of missing keys if this is not done!\n",
        "    ltable, rtable = load_known_dataset(dataset_name)\n",
        "\n",
        "    #Load compressed file and set Magellan's properties\n",
        "    candset_df = pd.read_pickle(input_file_name, compression=\"gzip\")\n",
        "\n",
        "    em.set_key(candset_df,'_id')\n",
        "    em.set_property(candset_df,'fk_ltable',\"ltable_id\")\n",
        "    em.set_property(candset_df,'fk_rtable',\"rtable_id\")\n",
        "    em.set_property(candset_df,'ltable',ltable)\n",
        "    em.set_property(candset_df,'rtable',rtable)\n",
        "\n",
        "    return candset_df\n",
        "\n",
        "#assumes the names of foreign keys as ltable_id and rtable_id\n",
        "def verify_blocking_ground_truth(dataset_name, block_df, objectify=False):\n",
        "    dataset_dtls = er_dataset_details[dataset_name]\n",
        "    all_matches_csv_file_name = dataset_dtls[\"dataset_folder_path\"] + dataset_dtls[\"golden_label_file_name\"]\n",
        "    duplicates_df = pd.read_csv(all_matches_csv_file_name)\n",
        "\n",
        "    num_duplicates_missed = 0\n",
        "    duplicates_df.columns = [\"ltable_id\", \"rtable_id\"]\n",
        "    #Sometimes pandas / Magellan puts some columns as objects instead of numeric/string. In this case, we will force this to join appropriately\n",
        "    if objectify:\n",
        "        duplicates_df = duplicates_df.astype(object)\n",
        "\n",
        "    #Intuition: merge function joints two data frames. The outer option creates a number of NaN rows when\n",
        "    # some duplicates are missing in the blocked_df\n",
        "    # we leverage the fact that len gives all rows while count gives non-NaN to compute the missing options\n",
        "    merged_df = block_df.merge(duplicates_df, left_on=[\"ltable_id\", \"rtable_id\"], right_on=[\"ltable_id\", \"rtable_id\"], how='outer')\n",
        "    num_duplicates_missed = len(merged_df) - merged_df[\"_id\"].count()\n",
        "    total_duplicates = len(duplicates_df)\n",
        "\n",
        "    print(\"Size of candset =\", len(block_df))\n",
        "    print(\"Totally missed:\", num_duplicates_missed, \" out of \", total_duplicates)\n",
        "\n",
        "\n",
        "def block_fodors_zagats(A, B):\n",
        "    ob = em.OverlapBlocker()\n",
        "    #No misses\n",
        "    C = ob.block_tables(A, B, 'name', 'name', l_output_attrs=['name', 'addr', 'city', 'phone'],  r_output_attrs=['name', 'addr', 'city', 'phone'],\n",
        "            overlap_size=1, show_progress=False)\n",
        "    return C\n",
        "\n",
        "\n",
        "def block_dblp_acm(A, B):\n",
        "    ab = em.AttrEquivalenceBlocker()\n",
        "    C = ab.block_tables(A, B, l_block_attr='year', r_block_attr='year', l_output_attrs=[\"title\",\"authors\",\"venue\",\"year\"],\n",
        "\t\tr_output_attrs=[\"title\",\"authors\",\"venue\",\"year\"], allow_missing=True)\n",
        "\n",
        "    ob = em.OverlapBlocker()\n",
        "\n",
        "    #=================>results in a candidate set of size 200K with no missing duplicates out of 2224\n",
        "    #C1 = ob.block_candset(C, 'title', 'title', word_level=True, overlap_size=1, show_progress=True)\n",
        "\n",
        "    #=================>results in a candidate set of size 46K with 5 missing duplicates out of 2224\n",
        "    C2 = ob.block_candset(C, 'title', 'title', word_level=True, overlap_size=2, show_progress=True)\n",
        "    return C2\n",
        "\n",
        "\n",
        "def block_amazon_googleproducts(A, B):\n",
        "    ob = em.OverlapBlocker()\n",
        "    #=================>results in a candidate set of size 400K with 6 missing duplicates out of 1300\n",
        "    C = ob.block_tables(A, B, \"title\", \"title\", word_level=True, overlap_size=1, l_output_attrs=[\"title\",\"description\",\"manufacturer\",\"price\"], r_output_attrs=[\"title\",\"description\",\"manufacturer\",\"price\"], show_progress=True, allow_missing=True)\n",
        "    return C\n",
        "\n",
        "\n",
        "def block_abt_buy(A, B):\n",
        "    B[\"description\"] = B[\"description\"] + \" \" + B[\"manufacturer\"]\n",
        "    ob = em.OverlapBlocker()\n",
        "    #=================>results in a candidate set of size 164K with 6 missing duplicates out of 1097\n",
        "    C = ob.block_tables(A, B, \"name\", \"name\", word_level=True, overlap_size=1,\n",
        "\tl_output_attrs=[\"name\",\"description\",\"price\"], r_output_attrs=[\"name\",\"description\",\"price\"], show_progress=True, allow_missing=True)\n",
        "    return C\n",
        "\n",
        "def block_walmart_amazon(A, B):\n",
        "    #assumes some preprocessing is done:\n",
        "    #Specifically in amazon.csv : a.\tpcategory2  => groupname , b.\t{ proddescrshort,proddescrlong } => shortdescr,longdescr\n",
        "\n",
        "    ob = em.OverlapBlocker()\n",
        "\n",
        "    #C1 = ob.block_tables(ltable, rtable, 'title', 'title', word_level=True, overlap_size=2)\n",
        "    #=================>results in a candidate set of size 1.1M with 20 missing duplicates out of 1154\n",
        "    #blocking_utils.verify_blocking_ground_truth(dataset_name, C1)\n",
        "\n",
        "    attributes = ['brand', 'groupname', 'title', 'price', 'shortdescr', 'longdescr', 'imageurl', 'modelno', 'shipweight', 'dimensions']\n",
        "    C2 = ob.block_tables(A, B, 'title', 'title', word_level=True, overlap_size=3, l_output_attrs=attributes, r_output_attrs=attributes)\n",
        "    #=================>results in a candidate set of size 278K with 84 missing duplicates out of 1154\n",
        "    #blocking_utils.verify_blocking_ground_truth(dataset_name, C2)\n",
        "\n",
        "    return C2\n",
        "\n",
        "def block_dblp_scholar(A, B):\n",
        "    ob = em.OverlapBlocker()\n",
        "    attributes = [\"id\",\"title\",\"authors\",\"venue\",\"year\"]\n",
        "    #C1 = ob.block_tables(A, B, 'title', 'title', word_level=True, overlap_size=3, show_progress=True, l_output_attrs=attributes, r_output_attrs=attributes)\n",
        "    #=================>results in a candidate set of size 1.2M with 178 missing duplicates out of 5347\n",
        "    C2 = ob.block_tables(A, B, 'title', 'title', word_level=True, overlap_size=4, show_progress=True, l_output_attrs=attributes, r_output_attrs=attributes)\n",
        "    #=================>results in a candidate set of size 135K with 467 missing duplicates out of 5347\n",
        "    return C2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PvMtViAXS0V",
        "colab_type": "text"
      },
      "source": [
        "### LSH"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jp2YQqLjXcHn",
        "colab_type": "code",
        "outputId": "e820d203-4e0c-4606-adef-487c88861acd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "'''\n",
        "Copyright (c) 2011, Yahoo! Inc.\n",
        "All rights reserved.\n",
        "\n",
        "Redistribution and use of this software in source and binary forms,\n",
        "with or without modification, are permitted provided that the following\n",
        "conditions are met:\n",
        "\n",
        "* Redistributions of source code must retain the above\n",
        "  copyright notice, this list of conditions and the\n",
        "  following disclaimer.\n",
        "\n",
        "* Redistributions in binary form must reproduce the above\n",
        "  copyright notice, this list of conditions and the\n",
        "  following disclaimer in the documentation and/or other\n",
        "  materials provided with the distribution.\n",
        "\n",
        "* Neither the name of Yahoo! Inc. nor the names of its\n",
        "  contributors may be used to endorse or promote products\n",
        "  derived from this software without specific prior\n",
        "  written permission of Yahoo! Inc.\n",
        "\n",
        "THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS\n",
        "IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED\n",
        "TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A\n",
        "PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n",
        "OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n",
        "SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n",
        "LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n",
        "DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n",
        "THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n",
        "(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
        "OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
        "'''\n",
        "\n",
        "# Python implementation of Andoni's e2LSH.  This version is fast because it\n",
        "# uses Python hashes to implement the buckets.  The numerics are handled\n",
        "# by the numpy routine so this should be close to optimal in speed (although\n",
        "# there is no control of the hash tables layout in memory.)\n",
        "\n",
        "# This file implements the following classes\n",
        "#\tlsh - the basic projection algorithm (on k-dimensional hash)\n",
        "#\tindex - a group of L lsh hashes\n",
        "#\tTestDataClass - a generic class for handling the raw data\n",
        "\n",
        "# To use\n",
        "#\tCall this routine with the -histogram flag to create some random\n",
        "#\t\ttest data and to compute the nearest-neighbor distances\n",
        "#\tLoad the .distance file that is produced into Matlab and compute\n",
        "#\t\tthe d_nn and d_any histograms from the first and second columns\n",
        "#\t\tof the .distance data.\n",
        "#\tUse these histograms (and their bin positions) as input to the\n",
        "#\t\tMatlab ComputeMPLSHParameters() routine.\n",
        "#\tThis gives you the optimum LSH parameters.  You can use these\n",
        "#\t\tvalues directly as parameters to this code.\n",
        "#\tYou can use the -ktest, -ltest and -wtest flags to test the\n",
        "#\t\tparameters.\n",
        "\n",
        "# Prerequisites: Python version 2.6 (2.5 might work) and NumPy\n",
        "\n",
        "# By Malcolm Slaney, Yahoo! Research\n",
        "\n",
        "import random, numpy, pickle, os, operator, traceback, sys, math, time\n",
        "import itertools\t\t\t\t\t# For Multiprobe\n",
        "\n",
        "#######################################################################\n",
        "# Note, the data is always a numpy array of size Dx1.\n",
        "#######################################################################\n",
        "\n",
        "# This class just implements k-projections into k integers\n",
        "# (after quantization) and then reducing that integer vector\n",
        "# into a T1 and T2 hash.  Data can either be entered into a\n",
        "# table, or retrieved.\n",
        "\n",
        "class lsh:\n",
        "\t'''This class implements one k-dimensional projection, the T1/T2 hashing\n",
        "\tand stores the results in a table for later retrieval.  Input parameters\n",
        "\tare the bin width (w, floating point, or float('inf') to get binary LSH),\n",
        "\tand the number of projections to compute for one table entry (k, an integer).'''\n",
        "\tdef __init__(self, w, k):\n",
        "\t\tself.k = k\t# Number of projections\n",
        "\t\tself.w = w\t# Bin width\n",
        "\t\tself.projections = None\n",
        "\t\tself.buckets = {}\n",
        "\n",
        "\t# This only works for Python >= 2.6\n",
        "\tdef sizeof(self):\n",
        "\t\t'''Return how much storage is needed for this object. In bytes\n",
        "\t\t'''\n",
        "\t\treturn sys.getsizeof(self.buckets) + \\\n",
        "\t\t\tsys.getsizeof(self.projections) + \\\n",
        "\t\t\tsys.getsizeof(self)\n",
        "\n",
        "\t# Create the random constants needed for the projections.\n",
        "\t# Can't do this until we see some data, so we know the\n",
        "\t# diementionality.\n",
        "\tdef CreateProjections(self, dim):\n",
        "\t\tself.dim = dim\n",
        "\t\t# print \"CreateProjections: Creating projection matrix for %dx%d data.\" % (self.k, self.dim)\n",
        "\t\tself.projections = numpy.random.randn(self.k, self.dim)\n",
        "\t\tself.bias = numpy.random.rand(self.k, 1)\n",
        "\t\tif 0:\n",
        "\t\t\tprint(\"Dim is\", self.dim)\n",
        "\t\t\tprint('Projections:\\n', self.projections)\n",
        "\t\t\t# print 'T1 hash:\\n', self.t1hash\n",
        "\t\t\t# print 'T2 hash:\\n', self.t2hash\n",
        "\t\tif 0:\n",
        "\t\t\t# Write out the project data so we can check it's properties.\n",
        "\t\t\t# Should be Gaussian with mean of zero and variance of 1.\n",
        "\t\t\tfp = open('Projections.data', 'w')\n",
        "\t\t\tfor i in xrange(0,self.projections.shape[0]):\n",
        "\t\t\t\tfor j in xrange(0,self.projections.shape[1]):\n",
        "\t\t\t\t\tfp.write('%g ' % self.projections[i,j])\n",
        "\t\t\t\tfp.write('\\n')\n",
        "\n",
        "\t# Compute the t1 and t2 hashes for some data.  Doing it this way\n",
        "\t# instead of in a loop, as before, is 10x faster.  Thanks to Anirban\n",
        "\t# for pointing out the flaw.  Not sure if the T2 hash is needed since\n",
        "\t# our T1 hash is so strong.\n",
        "\tdebugFP = None\n",
        "\tfirstTimeCalculateHashes = False\t\t# Change to false to turn this off\n",
        "\tinfinity = float('inf')\t\t\t\t\t# Easy way to access this flag\n",
        "\tdef CalculateHashes(self, data):\n",
        "\t\t'''Multiply the projection data (KxD) by some data (Dx1),\n",
        "\t\tand quantize'''\n",
        "                #Sara: changed self.projections == None to self.projections is None\n",
        "\t\tif self.projections is None:\n",
        "\t\t\tself.CreateProjections(len(data))\n",
        "\t\tbins = numpy.zeros((self.k,1), 'int')\n",
        "\t\tif lsh.firstTimeCalculateHashes:\n",
        "\t\t\tprint('data = ', numpy.transpose(data))\n",
        "\t\t\tprint('bias = ', numpy.transpose(self.bias))\n",
        "\t\t\tprint('projections = ',)\n",
        "\t\t\tfor i in range(0, self.projections.shape[0]):\n",
        "\t\t\t\tfor j in range(0, self.projections.shape[1]):\n",
        "\t\t\t\t\tprint(self.projections[i][j]),\n",
        "\t\t\t\tprint()\n",
        "\t\t\t# print 't1Hash = ', self.t1hash\n",
        "\t\t\t# print 't2Hash = ', self.t2hash\n",
        "\t\t\tfirstTimeCalculateHashes = False\n",
        "\t\t\tprint(\"Bin values:\", self.bias + \\\n",
        "\t\t\t\t\t\t\tnumpy.dot(self.projections, data)/self.w)\n",
        "\t\t\tprint(\"Type of bins:\", type(self.bias + \\\n",
        "\t\t\t\t\t\t\tnumpy.dot(self.projections, data)/self.w))\n",
        "\t\tif 0:\n",
        "\t\t\tif lsh.debugFP == None:\n",
        "\t\t\t\tprint(\"Opening Projections file\")\n",
        "\t\t\t\tlsh.debugFP = open('Projections.data', 'w')\n",
        "\t\t\td = self.bias + numpy.dot(self.projections, data)/self.w\n",
        "\t\t\tfor i in xrange(0, len(d)):\n",
        "\t\t\t\tlsh.debugFP.write('%g\\n' % d[i])\n",
        "\t\t\tlsh.debugFP.write('\\n')\n",
        "\t\t\tlsh.debugFP.flush()\n",
        "\t\tif self.w == lsh.infinity:\n",
        "\t\t\t# Binary LSH\n",
        "\t\t\tbins[:] = (numpy.sign(numpy.dot(self.projections, data))+1)/2.0\n",
        "\t\telse:\n",
        "\t\t\tbins[:] = numpy.floor(self.bias + numpy.dot(self.projections, data)/self.w)\n",
        "\t\tt1 = self.ListHash(bins)\n",
        "\t\tt2 = self.ListHash(bins[::-1])\t\t# Reverse data for second hash\n",
        "\t\treturn t1, t2\n",
        "\n",
        "\t# Input: A Nx1 array (of integers)\n",
        "\t# Output: A 28 bit hash value.\n",
        "\t# From: http://stackoverflow.com/questions/2909106/\n",
        "\t#\tpython-whats-a-correct-and-good-way-to-implement-hash/2909572#2909572\n",
        "\tdef ListHash(self, d):\n",
        "\t\t# return str(d).__hash__()\t\t# Good for testing, but not efficient\n",
        "                #Sara: changed d == None to d is None\n",
        "\t\tif d is None or len(d) == 0:\n",
        "\t\t\treturn 0\n",
        "\t\t# d = d.reshape((d.shape[0]*d.shape[1]))\n",
        "\t\tvalue = d[0, 0] << 7\n",
        "\t\tfor i in d[:,0]:\n",
        "\t\t\tvalue = (101*value + i)&0xfffffff\n",
        "\t\treturn value\n",
        "\n",
        "\t# Just a debug version that returns the bins too.\n",
        "\tdef CalculateHashes2(self, data):\n",
        "\t\tif self.projections == None:\n",
        "\t\t\tprint(\"CalculateHashes2: data.shape=%s, len(data)=%d\" % (str(data.shape), len(data)))\n",
        "\t\t\tself.CreateProjections(len(data))\n",
        "\t\tbins = numpy.zeros((self.k,1), 'int')\n",
        "\t\tparray = numpy.dot(self.projections, data)\n",
        "\t\tbins[:] = numpy.floor(parray/self.w  + self.bias)\n",
        "\t\tt1 = self.ListHash(bins)\n",
        "\t\tt2 = self.ListHash(bins[::-1])\t\t# Reverse data for second hash\n",
        "\t\t# print self.projections, data, parray, bins\n",
        "\t\t# sys.exit(1)\n",
        "\t\treturn t1, t2, bins, parray\n",
        "\n",
        "\t# Return a bunch of hashes, depending on the level of multiprobe\n",
        "\t# asked for.  Each list entry contains T1, T2. This is a Python\n",
        "\t# iterator... so call it in a for loop.  Each iteration returns\n",
        "\t# a bin ID (t1,t2)\n",
        "\t# [Need to store bins in integer array so we don't convert to\n",
        "\t# longs prematurely and get the wrong hash!]\n",
        "\tdef CalculateHashIterator(self, data, multiprobeRadius=0):\n",
        "                #Sara: changed self.projections == None to self.projections is None\n",
        "\t\tif self.projections is None:\n",
        "\t\t\tself.CreateProjections(len(data))\n",
        "\t\tbins = numpy.zeros((self.k,1), 'int')\n",
        "\t\tdirectVector = numpy.zeros((self.k,1), 'int')\n",
        "\t\tnewProbe = numpy.zeros((self.k,1), 'int')\n",
        "\t\tif self.w == lsh.infinity:\n",
        "\t\t\tpoints = numpy.dot(self.projections, data)\n",
        "\t\t\tbins[:] = (numpy.sign(points)+1)/2.0\n",
        "\t\t\tdirectVector[:] = -numpy.sign(bins-0.5)\n",
        "\t\telse:\n",
        "\t\t\tpoints = numpy.dot(self.projections, data)/self.w + self.bias\n",
        "\t\t\tbins[:] = numpy.floor(points)\n",
        "\t\t\tdirectVector[:] = numpy.sign(points-numpy.floor(points)-0.5)\n",
        "\t\tt1 = self.ListHash(bins)\n",
        "\t\tt2 = self.ListHash(bins[::-1])\n",
        "\t\tyield (t1,t2)\n",
        "\t\tif multiprobeRadius > 0:\n",
        "\t\t\t# print \"Multiprobe points:\", points\n",
        "\t\t\t# print \"Multiprobe bin:\", bins\n",
        "\t\t\t# print \"Multiprobe direct:\", directVector\n",
        "\t\t\tdimensions = range(self.k)\n",
        "\t\t\tdeltaVector = numpy.zeros((self.k, 1), 'int')\t# Preallocate\n",
        "\t\t\tfor r in range(1, multiprobeRadius+1):\n",
        "\t\t\t\t# http://docs.python.org/library/itertools.html\n",
        "\t\t\t\tfor candidates in itertools.combinations(dimensions, r):\n",
        "\t\t\t\t\tdeltaVector *= 0\t\t\t\t\t\t# Start Empty\n",
        "\t\t\t\t\tdeltaVector[list(candidates), 0] = 1\t# Set some bits\n",
        "\t\t\t\t\tnewProbe[:] = bins + deltaVector*directVector\t# New probe\n",
        "\t\t\t\t\tt1 = self.ListHash(newProbe)\n",
        "\t\t\t\t\tt2 = self.ListHash(newProbe[::-1])\t\t# Reverse data for second hash\n",
        "\t\t\t\t\t# print \"Multiprobe probe:\",newProbe, t1, t2\n",
        "\t\t\t\t\tyield (t1,t2)\n",
        "\n",
        "\t# Put some data into the hash bucket for this LSH projection\n",
        "\tdef InsertIntoTable(self, id, data):\n",
        "\t\t(t1, t2) = self.CalculateHashes(data)\n",
        "\t\tif t1 not in self.buckets:\n",
        "\t\t\tself.buckets[t1] = {t2: [id]}\n",
        "\t\telse:\n",
        "\t\t\tif t2 not in self.buckets[t1]:\n",
        "\t\t\t\tself.buckets[t1][t2] = [id]\n",
        "\t\t\telse:\n",
        "\t\t\t\tself.buckets[t1][t2].append(id)\n",
        "\n",
        "\t# Find some data in the hash bucket.  Return all the ids\n",
        "\t# that we find for this T1-T2 pair.\n",
        "\tdef FindXXObsolete(self, data):\n",
        "\t\t(t1, t2) = self.CalculateHashes(data)\n",
        "\t\tif t1 not in self.buckets:\n",
        "\t\t\treturn []\n",
        "\t\trow = self.buckets[t1]\n",
        "\t\tif t2 not in row:\n",
        "\t\t\treturn []\n",
        "\t\treturn row[t2]\n",
        "\n",
        "\t#\n",
        "\tdef Find(self, data, multiprobeRadius=0):\n",
        "\t\t'''Find the points that are close to the query data.  Use multiprobe\n",
        "\t\tto also look in nearby buckets.'''\n",
        "\t\tres = []\n",
        "\t\tfor (t1,t2) in self.CalculateHashIterator(data, multiprobeRadius):\n",
        "\t\t\t# print \"Find t1:\", t1\n",
        "\t\t\tif t1 not in self.buckets:\n",
        "\t\t\t\tcontinue\n",
        "\t\t\trow = self.buckets[t1]\n",
        "\t\t\tif t2 not in row:\n",
        "\t\t\t\tcontinue\n",
        "\t\t\tres += row[t2]\n",
        "\t\treturn res\n",
        "\n",
        "\t# Create a dictionary showing all the buckets an ID appears in\n",
        "\tdef CreateDictionary(self, theDictionary, prefix):\n",
        "\t\tfor b in self.buckets:\t\t# Over all buckets\n",
        "\t\t\tw = prefix + str(b)\n",
        "\t\t\tfor c in self.buckets[b]:# Over all T2 hashes\n",
        "\t\t\t\tfor i in self.buckets[b][c]:#Over ids\n",
        "\t\t\t\t\tif not i in theDictionary:\n",
        "\t\t\t\t\t\ttheDictionary[i] = [w]\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\ttheDictionary[i] += w\n",
        "\t\treturn theDictionary\n",
        "\n",
        "\n",
        "\t# Print some stats for these lsh buckets\n",
        "\tdef StatsXXX(self):\n",
        "\t\tmaxCount = 0; sumCount = 0;\n",
        "\t\tnumCount = 0; bucketLens = [];\n",
        "\t\tfor b in self.buckets:\n",
        "\t\t\tfor c in self.buckets[b]:\n",
        "\t\t\t\tl = len(self.buckets[b][c])\n",
        "\t\t\t\tif l > maxCount:\n",
        "\t\t\t\t\tmaxCount = l\n",
        "\t\t\t\t\tmaxLoc = (b,c)\n",
        "\t\t\t\t\t# print b,c,self.buckets[b][c]\n",
        "\t\t\t\tsumCount += l\n",
        "\t\t\t\tnumCount += 1\n",
        "\t\t\t\tbucketLens.append(l)\n",
        "\t\ttheValues = sorted(bucketLens)\n",
        "\t\tmed = theValues[(len(theValues)+1)/2-1]\n",
        "\t\tprint(\"Bucket Counts:\")\n",
        "\t\tprint(\"\\tTotal indexed points:\", sumCount)\n",
        "\t\tprint(\"\\tT1 Buckets filled: %d/%d\" % (len(self.buckets), 0))\n",
        "\t\tprint(\"\\tT2 Buckets used: %d/%d\" % (numCount, 0))\n",
        "\t\tprint(\"\\tMaximum T2 chain length:\", maxCount, \"at\", maxLoc)\n",
        "\t\tprint(\"\\tAverage T2 chain length:\", float(sumCount)/numCount)\n",
        "\t\tprint(\"\\tMedian T2 chain length:\", med)\n",
        "\n",
        "\tdef HealthStats(self):\n",
        "\t\t'''Count the number of points in each bucket (which is currently\n",
        "\t\ta function of both T1 and T2)'''\n",
        "\t\tmaxCount = 0; numCount = 0; totalIndexPoints = 0;\n",
        "\t\tfor b in self.buckets:\n",
        "\t\t\tfor c in self.buckets[b]:\n",
        "\t\t\t\tl = len(self.buckets[b][c])\n",
        "\t\t\t\tif l > maxCount:\n",
        "\t\t\t\t\tmaxCount = l\n",
        "\t\t\t\t\tmaxLoc = (b,c)\n",
        "\t\t\t\t\t# print b,c,self.buckets[b][c]\n",
        "\t\t\t\ttotalIndexPoints += l\n",
        "\t\t\t\tnumCount += 1\n",
        "\t\tT1Buckets = len(self.buckets)\n",
        "\t\tT2Buckets = numCount\n",
        "\t\tT1T2BucketAverage = totalIndexPoints/float(numCount)\n",
        "\t\tT1T2BucketMax = maxCount\n",
        "\t\treturn (T1Buckets, T2Buckets, T1T2BucketAverage, T1T2BucketMax)\n",
        "\n",
        "\t# Get a list of all IDs that are contained in these hash buckets\n",
        "\tdef GetAllIndices(self):\n",
        "\t\ttheList = []\n",
        "\t\tfor b in self.buckets:\n",
        "\t\t\tfor c in self.buckets[b]:\n",
        "\t\t\t\ttheList += self.buckets[b][c]\n",
        "\t\treturn theList\n",
        "\n",
        "\t# Put some data into the hash table, see how many collisions we get.\n",
        "\tdef Test(self, n):\n",
        "\t\tself.buckets = {}\n",
        "\t\tself.projections = None\n",
        "\t\td = numpy.array([.2,.3])\n",
        "\t\tfor i in range(0,n):\n",
        "\t\t\tself.InsertIntoTable(i, d+i)\n",
        "\t\tfor i in range(0,n):\n",
        "\t\t\tr = self.Find(d+i)\n",
        "\t\t\tmatches = sum(map(lambda x: x==i, r))\n",
        "\t\t\tif matches == 0:\n",
        "\t\t\t\tprint(\"Couldn't find item\", i)\n",
        "\t\t\telif matches == 1:\n",
        "\t\t\t\tpass\n",
        "\t\t\tif len(r) > 1:\n",
        "\t\t\t\tprint(\"Found big bin for\", i,\":\", r)\n",
        "\n",
        "\n",
        "# Put together several LSH projections to form an index.  The only\n",
        "# new parameter is the number of groups of projections (one LSH class\n",
        "# object per group.)\n",
        "class index:\n",
        "\tdef __init__(self, w, k, l):\n",
        "\t\tself.k = k;\n",
        "\t\tself.l = l\n",
        "\t\tself.w = w\n",
        "\t\tself.projections = []\n",
        "\t\tself.myIDs = []\n",
        "\t\tfor i in range(0,l):\t# Create all LSH buckets\n",
        "\t\t\tself.projections.append(lsh(w, k))\n",
        "\n",
        "\t# Only works for Python > 2.6\n",
        "\tdef sizeof(self):\n",
        "\t\t'''Return the sizeof this index in bytes.\n",
        "\t\t'''\n",
        "\t\treturn sum(p.sizeof() for p in self.projections) + \\\n",
        "\t\t\tsys.getsizeof(self)\n",
        "\n",
        "\t# Replace id we are given with a numerical id.  Since we are going\n",
        "\t# to use the ID in L tables, it is better to replace it here with\n",
        "\t# an integer.  We store the original ID in an array, and return it\n",
        "\t# to the user when we do a find().\n",
        "\tdef AddIDToIndex(self, id):\n",
        "\t\tif type(id) == int:\n",
        "\t\t\treturn id\t\t\t\t# Don't bother if already an int\n",
        "\t\tself.myIDs.append(id)\n",
        "\t\treturn len(self.myIDs)-1\n",
        "\n",
        "\tdef FindID(self, id):\n",
        "\t\tif type(id) != int or id < 0 or id >= len(self.myIDs):\n",
        "\t\t\treturn id\n",
        "\t\treturn self.myIDs[id]\n",
        "\n",
        "\t# Insert some data into all LSH buckets\n",
        "\tdef InsertIntoTable(self, id, data):\n",
        "\t\tintID = self.AddIDToIndex(id)\n",
        "\t\tfor p in self.projections:\n",
        "\t\t\tp.InsertIntoTable(intID, data)\n",
        "\n",
        "\tdef FindXXObsolete(self, data):\n",
        "\t\t'''Find some data in all the LSH buckets. Return a list of\n",
        "\t\tdata's id and bucket counts'''\n",
        "\t\titems = [p.Find(data) for p in self.projections]\n",
        "\t\tresults = {}\n",
        "\t\tfor itemList in items:\n",
        "\t\t\tfor item in itemList:\n",
        "\t\t\t\tif item in results:\t\t\t# Much faster without setdefault\n",
        "\t\t\t\t\tresults[item] += 1\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tresults[item] = 1\n",
        "\t\ts = sorted(results.items(), key=operator.itemgetter(1), \\\n",
        "\t\t\treverse=True)\n",
        "\t\treturn [(self.FindID(i),c) for (i,c) in s]\n",
        "\n",
        "\tdef Find(self, queryData, multiprobeR=0):\n",
        "\t\t'''Find some data in all the LSH tables.  Use Multiprobe, with\n",
        "\t\tthe given radius, to search neighboring buckets.  Return a list of\n",
        "\t\tresults.  Each result is a tuple consisting of the candidate ID\n",
        "\t\tand the number of times it was found in the index.'''\n",
        "\t\tresults = {}\n",
        "\t\tfor p in self.projections:\n",
        "\t\t\tids = p.Find(queryData, multiprobeR)\n",
        "\t\t\t# print \"Got back these IDs from p.Find:\", ids\n",
        "\t\t\tfor id in ids:\n",
        "\t\t\t\tif id in results:\n",
        "\t\t\t\t\tresults[id] += 1\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tresults[id] = 1\n",
        "\t\ts = sorted(results.items(), key=operator.itemgetter(1), \\\n",
        "\t\t\treverse=True)\n",
        "\t\treturn [(self.FindID(i),c) for (i,c) in s]\n",
        "\n",
        "\tdef FindExact(self, queryData, GetData, multiprobeR=0):\n",
        "\t\t'''Return a list of results sorted by their exact\n",
        "\t\tdistance from the query.  GetData is a function that\n",
        "\t\treturns the original data given its key.  This function returns\n",
        "\t\ta list of results, each result has the candidate ID and distance.'''\n",
        "\t\ts = self.Find(queryData, multiprobeR)\n",
        "\t\t# print \"Intermediate results are:\", s\n",
        "\t\td = map(lambda id,count: (id,((GetData(id)-queryData)**2).sum(), \\\n",
        "\t\t\t\tcount), s)\n",
        "\t\ts = sorted(d, key=operator.itemgetter(1))\n",
        "\t\treturn [(self.FindID(i),d) for (i,d,c) in s]\n",
        "\n",
        "\t# Put some data into the hash tables.\n",
        "\tdef Test(self, n):\n",
        "\t\td = numpy.array([.2,.3])\n",
        "\t\tfor i in range(0,n):\n",
        "\t\t\tself.InsertIntoTable(i, d+i)\n",
        "\t\tfor i in range(0,n):\n",
        "\t\t\tr = self.Find(d+i)\n",
        "\t\t\tprint(r)\n",
        "\n",
        "\t# Print the statistics of each hash table.\n",
        "\tdef Stats(self):\n",
        "\t\tfor i in range(0, len(self.projections)):\n",
        "\t\t\tp = self.projections[i]\n",
        "\t\t\tprint(\"Buckets\", i),\n",
        "\t\t\tp.Stats()\n",
        "\n",
        "\t# Get al the IDs that are part of this index.  Just check one hash\n",
        "\tdef GetAllIndices(self):\n",
        "\t\tif self.projections and len(self.projections) > 0:\n",
        "\t\t\tp = self.projections[0]\n",
        "\t\t\treturn p.GetAllIndices()\n",
        "\t\treturn None\n",
        "\n",
        "\t# Return the buckets (t1 and t2 hashes) associated with a data point\n",
        "\tdef GetBuckets(self, data):\n",
        "\t\tb = []\n",
        "\t\tfor p in self.projections:\n",
        "\t\t\t( t1, t2, bins, parray) = p.CalculateHashes2(data)\n",
        "\t\t\tprint(\"Bucket:\", t1, t2, bins, parray)\n",
        "\t\t\tb += (t1, t2)\n",
        "\t\treturn b\n",
        "\n",
        "\t#\n",
        "\tdef DictionaryPrefix(self, pc):\n",
        "\t\tprefix = 'W'\n",
        "\t\tprefixes = 'abcdefghijklmnopqrstuvwxyz'\n",
        "\t\twhile pc > 0:\t# Create unique ID for theis bucket\n",
        "\t\t\tprefix += prefixes[pc%len(prefixes)]\n",
        "\t\t\tpc /= len(prefixes)\n",
        "\t\treturn prefix\n",
        "\n",
        "\t# Create a list ordered by ID listing which buckets are used for each ID\n",
        "\tdef CreateDictionary(self):\n",
        "\t\ttheDictionary = {}\n",
        "\t\tpi = 0\n",
        "\t\tfor p in self.projections:\n",
        "\t\t\tprefix = self.DictionaryPrefix(pi)\n",
        "\t\t\ttheDictionary = p.CreateDictionary(theDictionary,\\\n",
        "\t\t\t\tprefix)\n",
        "\t\t\tpi += 1\n",
        "\t\treturn theDictionary\n",
        "\n",
        "\t# Find the bucket ids that best correspond to this piece of data.\n",
        "\tdef FindBuckets(self, data):\n",
        "\t\ttheWords = []\n",
        "\t\tpi = 0\n",
        "\t\tfor p in self.projections:\n",
        "\t\t\tprefix = self.DictionaryPrefix(pi)\n",
        "\t\t\t( t1, t2, bins, parray) = p.CalculateHashes2(data)\n",
        "\t\t\tword = prefix + str(t1)\n",
        "\t\t\ttheWords += [word]\n",
        "\t\t\tpi += 1\n",
        "\t\treturn theWords\n",
        "\n",
        "# Save an LSH index to a pickle file.\n",
        "def SaveIndex(filename, ind):\n",
        "\ttry:\n",
        "\t\tfp = open(filename, 'w')\n",
        "\t\tpickle.dump(ind, fp)\n",
        "\t\tfp.close()\n",
        "\t\tstatinfo = os.stat(filename,)\n",
        "\t\tif statinfo:\n",
        "\t\t\tprint(\"Wrote out\", statinfo.st_size, \"bytes to\", \\\n",
        "\t\t\t\tfilename)\n",
        "\texcept:\n",
        "\t\tprint(\"Couldn't pickle index to file\", filename)\n",
        "\t\ttraceback.print_exc(file=sys.stderr)\n",
        "\n",
        "# Read an LSH index from a pickle file.\n",
        "def LoadIndex(filename):\n",
        "\tif type(filename) == str:\n",
        "\t\ttry:\n",
        "\t\t\tfp = open(filename, 'r')\n",
        "\t\texcept:\n",
        "\t\t\tprint(\"Couldn't open %s to read LSH Index\" % (filename))\n",
        "\t\t\treturn None\n",
        "\telse:\n",
        "\t\tfp = filename\n",
        "\ttry:\n",
        "\t\tind = pickle.load(fp)\n",
        "\t\tfp.close()\n",
        "\t\treturn ind\n",
        "\texcept:\n",
        "\t\tprint(\"Couldn't read pickle file\", filename)\n",
        "\t\ttraceback.print_exc(file=sys.stderr)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class TestDataClass:\n",
        "\t'''A bunch of routines used to generate data we can use to test\n",
        "\tthis LSH implementation.'''\n",
        "\tdef __init__(self):\n",
        "\t\tself.myData = None\n",
        "\t\tself.myIndex = None\n",
        "\t\tself.nearestNeighbors = {}\t\t# A dictionary pointing to IDs\n",
        "\n",
        "\tdef LoadData(self, filename):\n",
        "\t\t'''Load data from a flat file, one line per data point.'''\n",
        "\t\tlineCount = 0\n",
        "\t\ttry:\n",
        "\t\t\tfp = open(filename)\n",
        "\t\t\tif fp:\n",
        "\t\t\t\tfor theLine in fp:\t\t\t# Count lines in file\n",
        "\t\t\t\t\tif theLine == '':\n",
        "\t\t\t\t\t\tbreak\n",
        "\t\t\t\t\tlineCount += 1\n",
        "\t\t\t\tdim = len(theLine.split())\t# Allocate the storage array\n",
        "\t\t\t\tself.myData = numpy.zeros((dim, lineCount))\n",
        "\t\t\t\tfp.seek(0,0)\t\t\t\t# Go back to beginning of file\n",
        "\t\t\t\tlineCount = 0\n",
        "\t\t\t\tfor theLine in fp:\t\t\t# Now load the data\n",
        "\t\t\t\t\tdata = [float(i) for i in theLine.split()]\n",
        "\t\t\t\t\tself.myData[:,lineCount] = data\n",
        "\t\t\t\t\tlineCount += 1\n",
        "\t\t\t\tfp.close()\n",
        "\t\t\telse:\n",
        "\t\t\t\tprint(\"Can't open %s to LoadData()\" % filename)\n",
        "\t\texcept:\n",
        "\t\t\tprint(\"Error loading data from %s in TestDataClass.LoadData()\" \\\n",
        "\t\t\t\t% filename)\n",
        "\t\t\ttraceback.print_exc(file=sys.stderr)\n",
        "\t\tprint(\"self.myData has %d lines and is:\" % lineCount, self.myData)\n",
        "\n",
        "\tdef SaveData(self, filename):\n",
        "\t\t'''Save this data in a flat file.  One line per data point.'''\n",
        "\t\tnumDims = self.NumDimensions()\n",
        "\t\ttry:\n",
        "\t\t\tfp = open(filename, 'w')\n",
        "\t\t\tif fp:\n",
        "\t\t\t\tfor i in xrange(0, self.NumPoints()):\n",
        "\t\t\t\t\tdata = self.RetrieveData(i).reshape(numDims)\n",
        "\t\t\t\t\tfp.write(' '.join([str(d) for d in data]) + '\\n')\n",
        "\t\t\t\tfp.close()\n",
        "\t\t\t\treturn\n",
        "\t\texcept:\n",
        "\t\t\tpass\n",
        "\t\tsys.stderr.write(\"Can't write test data to %s\\n\" % filename)\n",
        "\n",
        "\tdef CreateIndex(self, w, k, l):\n",
        "\t\t'''Create an index for the data we have in our database.  Inputs are\n",
        "\t\tthe LSH parameters: w, k and l.'''\n",
        "\t\tself.myIndex = index(w, k, l)\n",
        "\t\titemCount = 0\n",
        "\t\ttic = time.clock()\n",
        "\t\tfor itemID in self.IterateKeys():\n",
        "\t\t\tfeatures = self.RetrieveData(itemID)\n",
        "\t\t\tif features != None:\n",
        "\t\t\t\tself.myIndex.InsertIntoTable(itemID, features)\n",
        "\t\t\t\titemCount += 1\n",
        "\t\tprint(\"Finished indexing %d items in %g seconds.\" % \\\n",
        "\t\t\t(itemCount, time.clock()-tic))\n",
        "\t\tsys.stdout.flush()\n",
        "\n",
        "\tdef RetrieveData(self, id):\n",
        "\t\t'''Find a point in the array of data.'''\n",
        "\t\tid = int(id)\t\t\t\t\t\t# Key in this base class is an int!\n",
        "\t\tif id < self.myData.shape[1]:\n",
        "\t\t\treturn self.myData[:,id:id+1]\n",
        "\t\treturn None\n",
        "\n",
        "\tdef NumPoints(self):\n",
        "\t\t'''How many data point are in this database?'''\n",
        "\t\treturn self.myData.shape[1]\n",
        "\n",
        "\tdef NumDimensions(self):\n",
        "\t\t'''What is the dimensionality of the data?'''\n",
        "\t\treturn self.myData.shape[0]\n",
        "\n",
        "\tdef GetRandomQuery(self):\n",
        "\t\t'''Pick a random query from the dataset.  Return a key.'''\n",
        "\t\treturn random.randrange(0,self.NumPoints())\t# Pick random query\n",
        "\n",
        "\tdef FindNearestNeighbors(self, count):\n",
        "\t\t'''Exhaustive search for count nearest-neighbor results.\n",
        "\t\tSave the results in a dictionary.'''\n",
        "\t\tnumPoints = self.NumPoints()\n",
        "\t\tself.nearestNeighbors = {}\n",
        "\t\tfor i in xrange(0,count):\n",
        "\t\t\tqid = self.GetRandomQuery()\t\t\t\t# Pick random query\n",
        "\t\t\tqData = self.RetrieveData(qid)\t\t\t\t# Find it's data\n",
        "\t\t\tnearestDistance2 = None\n",
        "\t\t\tnearestIndex = None\n",
        "\t\t\tfor id2 in self.IterateKeys():\n",
        "\t\t\t\tif qid != id2:\n",
        "\t\t\t\t\td2 = ((self.RetrieveData(id2)-qData)**2).sum()\n",
        "\t\t\t\t\tif id == -1:\t\t\t\t\t# Debugging\n",
        "\t\t\t\t\t\tprint(qid, id2, qData, self.RetrieveData(id2), d2)\n",
        "\t\t\t\t\tif nearestDistance2 == None or d2 < nearestDistance2:\n",
        "\t\t\t\t\t\tnearestDistance2 = d2\n",
        "\t\t\t\t\t\tnearestIndex = id2\n",
        "\t\t\tself.nearestNeighbors[qid] = \\\n",
        "\t\t\t\t(nearestIndex, math.sqrt(nearestDistance2))\n",
        "\t\t\tif qid == -1:\n",
        "\t\t\t\tprint(qid, nearestIndex, math.sqrt(nearestDistance2))\n",
        "\t\t\t\tsys.stdout.flush()\n",
        "\n",
        "\tdef SaveNearestNeighbors(self, filename):\n",
        "\t\t'''Save the nearest neighbor dictionary in a file.  Each line\n",
        "\t\tof the file contains the query key, the distance to the nearest\n",
        "\t\tneighbor, and the NN key.'''\n",
        "\t\tif filename.endswith('.gz'):\n",
        "\t\t\timport gzip\n",
        "\t\t\tfp = gzip.open(filename, 'w')\n",
        "\t\telse:\n",
        "\t\t\tfp = open(filename, 'w')\n",
        "\t\tif fp:\n",
        "\t\t\tfor (query,(nn,dist)) in self.nearestNeighbors.items():\n",
        "\t\t\t\tfp.write('%s %g %s\\n' % (str(query), dist, str(nn)))\n",
        "\t\t\tfp.close()\n",
        "\t\telse:\n",
        "\t\t\tprint(\"Can't open %s to write nearest-neighbor data\" % filename)\n",
        "\n",
        "\tdef LoadNearestNeighbors(self, filename):\n",
        "\t\t'''Load a file full of nearest neighbor data.'''\n",
        "\t\tself.nearestNeighbors = {}\n",
        "\t\tif filename.endswith('.gz'):\n",
        "\t\t\timport gzip\n",
        "\t\t\tfp = gzip.open(filename, 'r')\n",
        "\t\telse:\n",
        "\t\t\tfp = open(filename, 'r')\n",
        "\t\tif fp:\n",
        "\t\t\tprint(\"Loading nearest-neighbor data from:\", filename)\n",
        "\t\t\tfor theLine in fp:\n",
        "\t\t\t\t(k,d,nn) = theLine.split()\n",
        "\t\t\t\tif type(self.myData) == numpy.ndarray: # Check for array indices\n",
        "\t\t\t\t\tk = int(k)\n",
        "\t\t\t\t\tnn = int(nn)\n",
        "\t\t\t\t\tif k < self.NumPoints() and nn < self.NumPoints():\n",
        "\t\t\t\t\t\tself.nearestNeighbors[k] = (nn,float(d))\n",
        "\t\t\t\telif k in self.myData and nn in self.myData:\t# dictionary index\n",
        "\t\t\t\t\tself.nearestNeighbors[k] = (nn,float(d))\n",
        "\t\t\tfp.close()\n",
        "\t\t\tprint(\" Loaded %d items into the nearest-neighbor dictionary.\" % len(self.nearestNeighbors))\n",
        "\t\telse:\n",
        "\t\t\tprint(\"Can't open %s to read nearest neighbor data.\" % filename)\n",
        "\n",
        "\tdef IterateKeys(self):\n",
        "\t\t'''Iterate through all possible keys in the dataset.'''\n",
        "\t\tfor i in range(self.NumPoints()):\n",
        "\t\t\tyield i\n",
        "\n",
        "\tdef FindMedian(self):\n",
        "\t\tnumDim = self.NumDimensions()\n",
        "\t\tnumPoints = self.NumPoints()\n",
        "\t\toneColumn = numpy.zeros((numPoints))\n",
        "\t\tmedians = numpy.zeros((numDim))\n",
        "\t\tfor d in xrange(numDim):\n",
        "\t\t\trowNumber = 0\n",
        "\t\t\tfor k in self.IterateKeys():\n",
        "\t\t\t\toneData = self.RetrieveData(k)\n",
        "\t\t\t\toneColumn[rowNumber] = oneData[d]\n",
        "\t\t\t\trowNumber += 1\n",
        "\t\t\tm = numpy.median(oneColumn, overwrite_input=True)\n",
        "\t\t\tmedians[d] = m\n",
        "\t\treturn medians\n",
        "\n",
        "\tdef ComputeDistanceHistogram(self, fp = sys.stdout):\n",
        "\t\t'''Calculate the nearest-neighbor and any-neighbor distance\n",
        "\t\thistograms needed for the LSH Parameter Optimization.  For\n",
        "\t\ta number of random query points, print the distance to the\n",
        "\t\tnearest neighbor, and to any random neighbor.  This becomes\n",
        "\t\tthe input for the parameter optimization routine.  Enhanced\n",
        "\t\tto also print the NN binary projections.'''\n",
        "\t\tnumPoints = self.NumPoints()\n",
        "\t\t# medians = self.FindMedian()\t\t# Not used now, but useful for binary quantization\n",
        "\t\tprint(\"Pulling %d items from the NearestNeighbors list for ComputeDistanceHistogram\" % \\\n",
        "\t\t\tlen(self.nearestNeighbors.items()))\n",
        "\t\tfor (queryKey,(nnKey,nnDist)) in self.nearestNeighbors.items():\n",
        "\t\t\trandKey = self.GetRandomQuery()\n",
        "\n",
        "\t\t\tqueryData = self.RetrieveData(queryKey)\n",
        "\t\t\tnnData = self.RetrieveData(nnKey)\n",
        "\t\t\trandData = self.RetrieveData(randKey)\n",
        "\t\t\tif len(queryData) == 0 or len(nnData) == 0:\t\t\t# Missing, probably because of subsampling\n",
        "\t\t\t\tprint(\"Skipping %s/%s because data is missing.\" % (queryKey, nnKey))\n",
        "\t\t\t\tcontinue\n",
        "\t\t\tanyD2 = ((randData-queryData)**2).sum()\n",
        "\n",
        "\t\t\tprojection = numpy.random.randn(1, queryData.shape[0])\n",
        "\t\t\t# print \"projection:\", projection.shape\n",
        "\t\t\t# print \"queryData:\", queryData.shape\n",
        "\t\t\t# print \"nnData:\", nnData.shape\n",
        "\t\t\t# print \"randData:\", randData.shape\n",
        "\t\t\tqueryProj = numpy.sign(numpy.dot(projection, queryData))\n",
        "\t\t\tnnProj = numpy.sign(numpy.dot(projection, nnData))\n",
        "\t\t\trandProj = numpy.sign(numpy.dot(projection, randData))\n",
        "\n",
        "\t\t\t# print 'CDH:', queryProj, nnProj, randProj\n",
        "\t\t\tfp.write('%g %g %d %d\\n' % \\\n",
        "\t\t\t\t(nnDist, math.sqrt(anyD2), \\\n",
        "\t\t\t\t queryProj==nnProj, queryProj==randProj))\n",
        "\t\t\tfp.flush()\n",
        "\n",
        "\tdef ComputePnnPany(self, w, k, l, multiprobe=0):\n",
        "\t\t'''Compute the probability of Pnn and Pany for a given index size.\n",
        "\t\tCreate the desired index, populate it with the data, and then measure\n",
        "\t\tthe NN and ANY neighbor retrieval rates.\n",
        "\t\tReturn\n",
        "\t\t\tthe pnn rate for one 1-dimensional index (l=1),\n",
        "\t\t\tthe pnn rate for an l-dimensional index,\n",
        "\t\t\tthe pany rate for one 1-dimensional index (l=1),\n",
        "\t\t\tand the pany rate for an l-dimensional index\n",
        "\t\t\tthe CPU time per query (seconds)'''\n",
        "\t\tnumPoints = self.NumPoints()\n",
        "\t\tnumDims = self.NumDimensions()\n",
        "\t\tself.CreateIndex(w, k, l)\t\t\t# Put data into new index\n",
        "\t\tcnn  = 0; cnnFull  = 0\n",
        "\t\tcany = 0; canyFull = 0\n",
        "\t\tqueryCount = 0\t\t\t\t\t\t\t# Probe the index\n",
        "\t\ttotalQueryTime = 0\n",
        "\t\tstartRecallTestTime = time.clock()\n",
        "\t\t# print \"ComputePnnPany: Testing %d nearest neighbors.\" % len(self.nearestNeighbors.items())\n",
        "\t\tfor (queryKey,(nnKey,dist)) in self.nearestNeighbors.items():\n",
        "\t\t\tqueryData = self.RetrieveData(queryKey)\n",
        "\t\t\tif queryData == None or len(queryData) == 0:\n",
        "\t\t\t\tprint(\"Can't find data for key %s\" % str(queryKey))\n",
        "\t\t\t\tsys.stdout.flush()\n",
        "\t\t\t\tcontinue\n",
        "\t\t\tstartQueryTime = time.clock()\t# Measure CPU time\n",
        "\t\t\tmatches = self.myIndex.Find(queryData, multiprobe)\n",
        "\t\t\ttotalQueryTime += time.clock() - startQueryTime\n",
        "\t\t\tfor (m,c) in matches:\n",
        "\t\t\t\tif nnKey == m:\t\t\t\t# See if NN was found!!!\n",
        "\t\t\t\t\tcnn += c\n",
        "\t\t\t\t\tcnnFull += 1\n",
        "\t\t\t\tif m != queryKey:\t\t\t# Don't count the query\n",
        "\t\t\t\t\tcany += c\n",
        "\t\t\tcanyFull += len(matches)-1\t\t# Total candidates minus 1 for query\n",
        "\t\t\tqueryCount += 1\n",
        "\t\t\t# Some debugging for k curve.. print individual results\n",
        "\t\t\t# print \"ComputePnnPany Debug:\", w, k, l, len(matches), numPoints, cnn, cnnFull, cany, canyFull\n",
        "\t\trecallTestTime = time.clock() - startRecallTestTime\n",
        "\t\tprint(\"Tested %d NN queries in %g seconds.\" % (queryCount, recallTestTime))\n",
        "\t\tsys.stdout.flush()\n",
        "\t\tif queryCount == 0:\n",
        "\t\t\tqueryCount = 1\t\t\t\t\t# To prevent divide by zero\n",
        "\t\tperQueryTime = totalQueryTime/queryCount\n",
        "\t\tprint(\"CPP:\", cnn, cnnFull, cany, canyFull)\n",
        "\t\tprint(\"CPP:\",  cnn/float(queryCount*l), cnnFull/float(queryCount), \\\n",
        "\t\t\tcany/float(queryCount*l*numPoints), canyFull/float(queryCount*numPoints), \\\n",
        "\t\t\tperQueryTime, numDims)\n",
        "\t\treturn cnn/float(queryCount*l), cnnFull/float(queryCount), \\\n",
        "\t\t\tcany/float(queryCount*l*numPoints), canyFull/float(queryCount*numPoints), \\\n",
        "\t\t\tperQueryTime, numDims\n",
        "\n",
        "\tdef ComputePnnPanyCurve(self, wList = .291032, multiprobe=0):\n",
        "\t\t\tif type(wList) == float or type(wList) == int:\n",
        "\t\t\t\twList = [wList*10**((i-10)/10.0) for i in range(0,21)]\n",
        "\t\t\tfor w in wList:\n",
        "\t\t\t\t(pnn, pnnFull, pany, panyFull, queryTime, numDims) = self.ComputePnnPany(w, 1, 10, multiprobe)\n",
        "\t\t\t\tif w == wList[0]:\n",
        "\t\t\t\t\tprint(\"# w pnn pany queryTime\")\n",
        "\t\t\t\tprint(\"PnnPany:\", w, multiprobe, pnn, pany, queryTime)\n",
        "\t\t\t\tsys.stdout.flush()\n",
        "\n",
        "\tdef ComputeKCurve(self, kList, w = .291032, r=0):\n",
        "\t\t'''Compute the number of ANY neighbors as a function of\n",
        "\t\tk.  Should go down exponentially.'''\n",
        "\t\tnumPoints = self.NumPoints()\n",
        "\t\tl = 10\n",
        "\t\tfor k in sorted(list(kList)):\n",
        "\t\t\t(pnn, pnnFull, pany, panyFull, queryTime, numDims) = self.ComputePnnPany(w, k, l, r)\n",
        "\t\t\tprint(w, k, l, r, pnn, pany, pany*numPoints, queryTime)\n",
        "\t\t\tsys.stdout.flush()\n",
        "\n",
        "\tdef ComputeLCurve(self, lList, w = 2.91032, k=10, r=0):\n",
        "\t\t'''Compute the probability of nearest neighbors as a function\n",
        "\t\tof l.'''\n",
        "\t\tnumPoints = self.NumPoints()\n",
        "\t\tfirstTime = True\n",
        "\t\tfor l in sorted(list(lList)):\n",
        "\t\t\t(pnn, pnnFull, pany, panyFull, queryTime, numDims) = self.ComputePnnPany(w, k, l, r)\n",
        "\t\t\tif firstTime:\n",
        "\t\t\t\tprint(\"# w k l r pnnFull, panyFull panyFull*N queryTime\")\n",
        "\t\t\t\tfirstTime = False\n",
        "\t\t\tprint(w, k, l, r, pnnFull, panyFull, panyFull*numPoints, queryTime)\n",
        "\t\t\tsys.stdout.flush()\n",
        "\n",
        "\n",
        "\n",
        "class RandomTestData(TestDataClass):\n",
        "\t'''Generate uniform random data points between -1 and 1.'''\n",
        "\tdef CreateData(self, numPoints, dim):\n",
        "\t\tself.myData = (numpy.random.rand(dim, numPoints)-.5)*2.0\n",
        "\n",
        "class HyperCubeTestData(TestDataClass):\n",
        "\t'''Create a hypercube of data.  All points are in the corners'''\n",
        "\tdef CreateData(self, numDim, noise = None):\n",
        "\t\tnumPoints = 2**numDim\n",
        "\t\tself.myData = numpy.zeros((numPoints, numDim))\n",
        "\t\tfor i in range(0,numPoints):\n",
        "\t\t\tfor b in range(0,numDim):\n",
        "\t\t\t\tif (2**b) & i:\n",
        "\t\t\t\t\tself.myData[b, i] = 1.0\n",
        "\t\tif noise != None:\n",
        "\t\t\tself.myData += (numpy.random.rand(numDim, numPoints)-.5)*noise\n",
        "\n",
        "class RegularTestData(TestDataClass):\n",
        "\t'''Fill the 2-D test array with a regular grid of points between -1 and 1'''\n",
        "\tdef CreateData(self, numDivs):\n",
        "\t\tself.myData = numpy.zeros(((2*numDivs+1)**2,2))\n",
        "\t\ti = 0\n",
        "\t\tfor x in range(-numDivs, numDivs+1):\n",
        "\t\t\tfor y in range(-numDivs, numDivs+1):\n",
        "\t\t\t\tself.myData[0, i] = x/float(divs)\n",
        "\t\t\t\tself.myData[1, i] = y/float(divs)\n",
        "\t\t\t\ti += 1\n",
        "\n",
        "\n",
        "\n",
        "# Use Dimension Doubling to measure the dimensionality of a random\n",
        "# set of data.  Generate some data (either random Gaussian or a grid)\n",
        "# Then count the number of points that fall within the given radius of this\n",
        "# query.\n",
        "def XXXTestDimensionality2():\n",
        "\tbinWidth = .5\n",
        "\tif True:\n",
        "\t\tnumPoints = 100000\n",
        "\t\tmyTestData = TestDataClass(numPoints, 3)\n",
        "\telse:\n",
        "\t\tmyTestData = RegularTestData(100)\n",
        "\t\tnumPoints = myTestData.NumPoints\n",
        "\tk = 4; l = 2; N = 1000\n",
        "\tmyTestIndex = index(binWidth, k, l, N)\n",
        "\tfor i in range(0,numPoints):\n",
        "\t\tmyTestIndex.InsertIntoTable(i, myTestData.RetrieveData(i))\n",
        "\trBig = binWidth/8.0\n",
        "\trSmall = rBig/2.0\n",
        "\tcBig = 0.0; cSmall = 0.0\n",
        "\tfor id in random.sample(ind.GetAllIndices(), 2):\n",
        "\t\tqp = FindLSHTestData(id)\n",
        "\t\tcBig += myTestIndex.CountInsideRadius(qp, myTestData.FindData, rBig)\n",
        "\t\tcSmall += myTestIndex.CountInsideRadius(qp, myTestData.FindData, rSmall)\n",
        "\tif cBig > cSmall and cSmall > 0:\n",
        "\t\tdim = math.log(cBig/cSmall)/math.log(rBig/rSmall)\n",
        "\telse:\n",
        "\t\tdim = 0\n",
        "\tprint(cBig, cSmall, dim)\n",
        "\treturn ind\n",
        "\n",
        "\n",
        "# Generate some 2-dimensional data, put it into an index and then\n",
        "# show the points retrieved.  This is all done as a function of number\n",
        "# of projections per bucket, number of buckets to use for each index, and\n",
        "# the number of LSH bucket (the T1 size).  Write out the data so we can\n",
        "# plot it (in Matlab)\n",
        "def GraphicalTest(k, l, N):\n",
        "\tnumPoints = 1000\n",
        "\tmyTestData = TestDataClass(numPoints, 3)\n",
        "\tind = index(.1, k, l, N)\n",
        "\tfor i in range(0,numPoints):\n",
        "\t\tind.InsertIntoTable(i, myTestData.RetrieveData(i))\n",
        "\ti = 42\n",
        "\tr = ind.Find(data[i,:])\n",
        "\tfp = open('lshtestpoints.txt','w')\n",
        "\tfor i in range(0,numPoints):\n",
        "\t\tif i in r:\n",
        "\t\t\tc = r[i]\n",
        "\t\telse:\n",
        "\t\t\tc = 0\n",
        "\t\tfp.write(\"%g %g %d\\n\" % (data[i,0], data[i,1], c))\n",
        "\tfp.close()\n",
        "\treturn r\n",
        "\n",
        "\n",
        "\n",
        "def SimpleTest():\n",
        "\timport time\n",
        "\tdim = 250\n",
        "\tnumPoints = 10000\n",
        "\tmyTestData = RandomTestData()\n",
        "\tmyTestData.CreateData(numPoints,dim)\n",
        "\tmyTestIndex = index(w=.4, k=10, l=10, N=numPoints)\n",
        "\tstartLoad = time.clock()\n",
        "\tfor id in myTestData.IterateKeys():\n",
        "\t\tdata = myTestData.RetrieveData(id)\n",
        "\t\tmyTestIndex.InsertIntoTable(id, data)\n",
        "\tendLoad = time.clock()\n",
        "\tprint(\"Time to load %d points is %gs (%gms per point)\" % \\\n",
        "\t\t(numPoints, endLoad-startLoad, (endLoad-startLoad)/numPoints*1000.0))\n",
        "\n",
        "\tstartRecall = time.clock()\n",
        "\tresCount = 0\n",
        "\tresFound = 0\n",
        "\tfor id in myTestData.IterateKeys():\n",
        "\t\tquery = myTestData.RetrieveData(id)\n",
        "\t\tres = myTestIndex.Find(query)\n",
        "\t\tif not res == None and len(res) > 0:\n",
        "\t\t\tresFound += 1\n",
        "\t\tif not res == None:\n",
        "\t\t\tresCount += len(res)\n",
        "\tendRecall = time.clock()\n",
        "\tprint(\"Time to recall %d points is %gs (%gms per point\" % \\\n",
        "\t\t(numPoints, endRecall-startRecall, (endRecall-startRecall)/numPoints*1000.0))\n",
        "\tprint(\"Found a recall hit all but %d times, average results per query is %g\" % \\\n",
        "\t\t(numPoints-resFound, resCount/float(numPoints)))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def OutputAllProjections(myTestData, myTestIndex, filename):\n",
        "\t'''Calculate and output all the projected data for an index.'''\n",
        "\tlshProjector = myTestIndex.projections[0]\n",
        "\tfp = open(filename, 'w')\n",
        "\tfor id in myTestData.IterateKeys():\n",
        "\t\td = myTestData.RetrieveData(id)\n",
        "\t\t(t1, t2, bins, parray) = lshProjector.CalculateHashes2(d)\n",
        "\t\tfp.write('%d %d %g %g\\n' % (t1, t2, bins[0][0], parray[0][0]))\n",
        "\tfp.close()\n",
        "\n",
        "# \tExact Optimization:\n",
        "#\t\tFor 100000 5-d data use: w=2.91032 and get 0.55401 hits per bin and 0.958216 nn.\n",
        "#\t\t\tK=23.3372 L=2.70766 cost is 2.98756\n",
        "#\tExpected statistics for optimal solution:\n",
        "#\t\tAssuming K=23, L=3\n",
        "#\t\tp_nn(w) is 0.958216\n",
        "#\t\tp_any(w) is 0.55401\n",
        "#\t\tProbability of finding NN for L=1: 0.374677\n",
        "#\t\tProbability of finding ANY for L=1: 1.26154e-06\n",
        "#\t\tProbability of finding NN for L=3: 0.75548\n",
        "#\t\tProbability of finding ANY for L=3: 3.78462e-06\n",
        "#\t\tExpected number of hits per query: 0.378462\n",
        "\n",
        "'''\n",
        "10-D data:\n",
        "Mean of Python NN data is 0.601529 and std is 0.0840658.\n",
        "Scaling all distances by 0.788576 for easier probability calcs.\n",
        "Simple Approximation:\n",
        "\tFor 100000 5-d data use: w=4.17052 and get 0.548534 hits per bin and 0.885004 nn.\n",
        "\t\tK=19.172 L=10.4033 cost is 20.8065\n",
        "Expected statistics: for simple approximation\n",
        "\tAssuming K=19, L=10\n",
        "\tProbability of finding NN for L=1: 0.0981652\n",
        "\tProbability of finding ANY for L=1: 1.10883e-05\n",
        "\tProbability of finding NN for L=10: 0.644148\n",
        "\tProbability of finding ANY for L=10: 0.000110878\n",
        "\tExpected number of hits per query: 11.0878\n",
        "Exact Optimization:\n",
        "\tFor 100000 5-d data use: w=4.26786 and get 0.556604 hits per bin and 0.887627 nn.\n",
        "\t\tK=21.4938 L=12.9637 cost is 17.3645\n",
        "Expected statistics for optimal solution:\n",
        "\tAssuming K=21, L=13\n",
        "\tp_nn(w) is 0.887627\n",
        "\tp_any(w) is 0.556604\n",
        "\tProbability of finding NN for L=1: 0.0818157\n",
        "\tProbability of finding ANY for L=1: 4.53384e-06\n",
        "\tProbability of finding NN for L=13: 0.670323\n",
        "\tProbability of finding ANY for L=13: 5.89383e-05\n",
        "\tExpected number of hits per query: 5.89383\n",
        "'''\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\tdefaultDims = 10\n",
        "\tdefaultW = 2.91032\n",
        "\tdefaultK = 10\n",
        "\tdefaultL = 1\n",
        "\tdefaultClosest = 1000\n",
        "\tdefaultMultiprobeRadius = 0\n",
        "\tdefaultFileName = 'testData'\n",
        "\tcmdName = sys.argv.pop(0)\n",
        "\twhile len(sys.argv) > 0:\n",
        "\t\targ = sys.argv.pop(0).lower()\n",
        "\t\tif arg == '-d':\n",
        "\t\t\targ = sys.argv.pop(0)\n",
        "\t\t\ttry:\n",
        "\t\t\t\tdefaultDims = int(arg)\n",
        "\t\t\t\tdefaultFileName = 'testData%03d' % defaultDims\n",
        "\t\t\texcept:\n",
        "\t\t\t\tprint(\"Couldn't parse new value for defaultDims: %s\" % arg)\n",
        "\t\t\tprint('New default dimensions for test is', defaultDims)\n",
        "\t\telif arg == '-f':\n",
        "\t\t\tdefaultFileName = sys.argv.pop(0)\n",
        "\t\t\tprint('New file name is', defaultFileName)\n",
        "\t\telif arg == '-k':\n",
        "\t\t\targ = sys.argv.pop(0)\n",
        "\t\t\ttry:\n",
        "\t\t\t\tdefaultK = int(arg)\n",
        "\t\t\texcept:\n",
        "\t\t\t\tprint(\"Couldn't parse new value for defaultK: %s\" % arg)\n",
        "\t\t\tprint('New default k for test is', defaultK)\n",
        "\t\telif arg == '-l':\n",
        "\t\t\targ = sys.argv.pop(0)\n",
        "\t\t\ttry:\n",
        "\t\t\t\tdefaultL = int(arg)\n",
        "\t\t\texcept:\n",
        "\t\t\t\tprint(\"Couldn't parse new value for defaultL: %s\" % arg)\n",
        "\t\t\tprint('New default l for test is', defaultL)\n",
        "\t\telif arg == '-c':\n",
        "\t\t\targ = sys.argv.pop(0)\n",
        "\t\t\ttry:\n",
        "\t\t\t\tdefaultClosest = int(arg)\n",
        "\t\t\texcept:\n",
        "\t\t\t\tprint(\"Couldn't parse new value for defaultClosest: %s\" % arg)\n",
        "\t\t\tprint('New default number closest for test is', defaultClosest)\n",
        "\t\telif arg == '-w':\n",
        "\t\t\targ = sys.argv.pop(0)\n",
        "\t\t\ttry:\n",
        "\t\t\t\tdefaultW = float(arg)\n",
        "\t\t\texcept:\n",
        "\t\t\t\tprint(\"Couldn't parse new value for w: %s\" % arg)\n",
        "\t\t\tprint('New default W for test is', defaultW)\n",
        "\t\telif arg == '-r':\n",
        "\t\t\targ = sys.argv.pop(0)\n",
        "\t\t\ttry:\n",
        "\t\t\t\tdefaultMultiprobeRadius = int(arg)\n",
        "\t\t\texcept:\n",
        "\t\t\t\tprint(\"Couldn't parse new value for multiprobeRadius: %s\" % arg)\n",
        "\t\t\tprint('New default multiprobeRadius for test is', defaultMultiprobeRadius)\n",
        "\t\telif arg == '-create':\t\t\t# Create some uniform random data and find NN\n",
        "\t\t\tmyTestData = RandomTestData()\n",
        "\t\t\tmyTestData.CreateData(100000, defaultDims)\n",
        "\t\t\tmyTestData.SaveData(defaultFileName + '.dat')\n",
        "\t\t\tprint(\"Finished creating random data.  Now computing nearest neighbors...\")\n",
        "\t\t\tmyTestData.FindNearestNeighbors(defaultClosest)\n",
        "\t\t\tmyTestData.SaveNearestNeighbors(defaultFileName + '.nn')\n",
        "\t\telif arg == '-histogram':\t\t# Calculate distance histograms\n",
        "\t\t\tmyTestData = TestDataClass()\n",
        "\t\t\tmyTestData.LoadData(defaultFileName + '.dat')\n",
        "\t\t\tmyTestData.LoadNearestNeighbors(defaultFileName + '.nn')\n",
        "\t\t\tfp = open(defaultFileName + '.distances', 'w')\n",
        "\t\t\tif fp:\n",
        "\t\t\t\tmyTestData.ComputeDistanceHistogram(fp)\n",
        "\t\t\t\tfp.close()\n",
        "\t\t\telse:\n",
        "\t\t\t\tprint(\"Can't open %s.distances to store NN data\" % defaultFileName)\n",
        "\t\telif arg == '-sanity':\n",
        "\t\t\tmyTestData = TestDataClass()\n",
        "\t\t\tmyTestData.LoadData(defaultFileName + '.dat')\n",
        "\t\t\tprint(myTestData.RetrieveData(myTestData.GetRandomQuery()))\n",
        "\t\t\tprint(myTestData.RetrieveData(myTestData.GetRandomQuery()))\n",
        "\t\telif arg == '-b':\t\t# Calculate bucket probabilities\n",
        "\t\t\trandom.seed(0)\n",
        "\t\t\tmyTestData = TestDataClass()\n",
        "\t\t\tmyTestData.LoadData(defaultFileName + '.dat')\n",
        "\t\t\tmyTestData.LoadNearestNeighbors(defaultFileName + '.nn')\n",
        "\t\t\t# ComputePnnPanyCurve(myData, [.291032])\n",
        "\t\t\tmyTestData.ComputePnnPanyCurve(defaultW)\n",
        "\t\telif arg == '-wtest':\t\t# Calculate bucket probabilities as a function of w\n",
        "\t\t\trandom.seed(0)\n",
        "\t\t\tmyTestData = TestDataClass()\n",
        "\t\t\tmyTestData.LoadData(defaultFileName + '.dat')\n",
        "\t\t\tmyTestData.LoadNearestNeighbors(defaultFileName + '.nn')\n",
        "\t\t\twList = [defaultW*.5**-i for i in range(-10,10)]\n",
        "\t\t\t# wList = [defaultW*.5**-i for i in range(-3,3)]\n",
        "\t\t\tmyTestData.ComputePnnPanyCurve(wList, defaultMultiprobeRadius)\n",
        "\t\telif arg == '-ktest':\t\t# Calculate bucket probabilities as a function of k\n",
        "\t\t\trandom.seed(0)\n",
        "\t\t\tmyTestData = TestDataClass()\n",
        "\t\t\tmyTestData.LoadData(defaultFileName + '.dat')\n",
        "\t\t\tmyTestData.LoadNearestNeighbors(defaultFileName + '.nn')\n",
        "\t\t\t# ComputePnnPanyCurve(myData, [.291032])\n",
        "\t\t\tkList = [math.floor(math.sqrt(2)**k) for k in range(0,10)]\n",
        "\t\t\tkList = [1,2,3,4,5,6,8,10,12,14,16,18,20,22,25,30,35,40]\n",
        "\t\t\tmyTestData.ComputeKCurve(kList, defaultW, defaultMultiprobeRadius)\n",
        "\t\telif arg == '-ltest':\t\t# Calculate bucket probabilities as a function of l\n",
        "\t\t\trandom.seed(0)\n",
        "\t\t\tmyTestData = TestDataClass()\n",
        "\t\t\tmyTestData.LoadData(defaultFileName + '.dat')\n",
        "\t\t\tmyTestData.LoadNearestNeighbors(defaultFileName + '.nn')\n",
        "\t\t\t# ComputePnnPanyCurve(myData, [.291032])\n",
        "\t\t\tlList = [math.floor(math.sqrt(2)**k) for k in range(0,10)]\n",
        "\t\t\tlList = [1,2,3,4,5,6,8,10,12,14,16,18,20,22,25,30]\n",
        "\t\t\tmyTestData.ComputeLCurve(lList, w=defaultW,\n",
        "\t\t\t\tk=defaultK, r=defaultMultiprobeRadius)\n",
        "\t\telif arg == '-timing':\n",
        "\t\t\t# sys.argv.pop(0)\n",
        "\t\t\ttimingModels = []\n",
        "\t\t\twhile len(sys.argv) > 0:\n",
        "\t\t\t\tprint(\"Parsing timing argument\", sys.argv[0], len(sys.argv))\n",
        "\t\t\t\tif sys.argv[0].startswith('-'):\n",
        "\t\t\t\t\tbreak\n",
        "\t\t\t\ttry:\n",
        "\t\t\t\t\t(w,k,l,r,rest) = sys.argv[0].strip().split(',', 5)\n",
        "\t\t\t\t\ttimingModels.append([float(w), int(k), int(l), int(r)])\n",
        "\t\t\t\texcept:\n",
        "\t\t\t\t\tprint(\"Couldn't parse %s.  Need w,k,l,r\" % sys.argv[0])\n",
        "\t\t\t\tsys.argv.pop(0)\n",
        "\t\t\tmyTestData = TestDataClass()\n",
        "\t\t\tmyTestData.LoadData(defaultFileName + '.dat')\n",
        "\t\t\tmyTestData.LoadNearestNeighbors(defaultFileName + '.nn')\n",
        "\t\t\tfor (w, k, l, r) in timingModels:\n",
        "\t\t\t\tsys.stdout.flush()\n",
        "\t\t\t\t(pnnL1, pnn, panyL1, pany, perQueryTime, numDims) = myTestData.ComputePnnPany(w, k, l, r)\n",
        "\t\t\t\tprint(\"Timing:\", w, k, l, r, myTestData.NumPoints(), pnn, pany, perQueryTime*1000.0, numDims)\n",
        "\n",
        "\t\telif arg == '-test':\t\t# Calculate bucket probabilities as a function of l\n",
        "\t\t\trandom.seed(0)\n",
        "\t\t\tmyTestData = TestDataClass()\n",
        "\t\t\tmyTestData.LoadData(defaultFileName + '.dat')\n",
        "\t\t\tmyTestData.LoadNearestNeighbors(defaultFileName + '.nn')\n",
        "\t\t\t# ComputePnnPanyCurve(myData, [.291032])\n",
        "\t\t\tmyTestData.ComputeLCurve([defaultL], w=defaultW, k=defaultK)\n",
        "\t\telse:\n",
        "\t\t\tprint('%s: Unknown test argument %s' % (cmdName, arg))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "New file name is /root/.local/share/jupyter/runtime/kernel-1c195346-6f0b-4a4f-9b23-1ad30712fbab.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdP40E-BXcXI",
        "colab_type": "text"
      },
      "source": [
        "### LSH Blocking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fn_vnMKLXfFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#GiG\n",
        "\n",
        "import csv\n",
        "import torchfile\n",
        "import numpy\n",
        "import random\n",
        "import time\n",
        "import sys\n",
        "\n",
        "\n",
        "#To ensure reproducibility - feel free to change them\n",
        "# Use 20 random seeds as LSH is very probabilistic\n",
        "# the values are got by sorted([random.randint(0, 100000) for gig in range(20)])\n",
        "#random_seeds = [778, 3527, 3647, 6917, 12539, 12684, 16778, 19300, 27564, 42453, 47877, 50564, 55767, 60546, 60885, 66032, 71986, 90605, 95046, 97852]\n",
        "random_seeds = [778, 3527, 3647, 6917, 12539, 12684, 16778, 19300, 27564, 42453]\n",
        "\n",
        "\n",
        "\n",
        "def get_csv_reader(csv_file_name):\n",
        "    f = open(csv_file_name)\n",
        "    reader = csv.reader(f)\n",
        "    next(reader)\n",
        "    return reader\n",
        "\n",
        "torch_id_to_vec_hash = {}\n",
        "\n",
        "def id_to_vec(id_val):\n",
        "    return torch_id_to_vec_hash[id_val]\n",
        "\n",
        "def get_torch_data_as_dicts(torch_file_name):\n",
        "    global torch_id_to_vec_hash\n",
        "\n",
        "    data = torchfile.load(torch_file_name)\n",
        "    dict_id_to_index = {}\n",
        "    dict_vec_to_id = {}\n",
        "    for i in range(len(data)):\n",
        "        id_val = data[i]['id']\n",
        "        dict_id_to_index[id_val] = i\n",
        "        torch_id_to_vec_hash[id_val] = data[i]['vec']\n",
        "    return data, dict_id_to_index\n",
        "\n",
        "#No need to index both datasets\n",
        "#Index the smaller one and query the bigger one\n",
        "def index_data(lsh_engine, data, dataset1_size, dimension=300):\n",
        "    for i in range(dataset1_size):\n",
        "        #Sara: the following doesnt seem to work after some numpy update\n",
        "        #vector = data[i]['vec'].reshape((dimension, 1))\n",
        "        vector = data[i]['vec']\n",
        "        vector.shape = (dimension, 1)\n",
        "        id_val = data[i]['id']\n",
        "        lsh_engine.InsertIntoTable(id_val, vector)\n",
        "\n",
        "def create_lsh(K, L):\n",
        "    #create L projects of size K each\n",
        "    lsh_engine = lsh.index(float('inf'), K, L)\n",
        "    return lsh_engine\n",
        "\n",
        "def query_data_binary(lsh_engine, query_vec, match_id):\n",
        "    matches = lsh_engine.Find(query_vec)\n",
        "    tuple_ids = [elem[0] for elem in matches]\n",
        "    return match_id in tuple_ids\n",
        "\n",
        "def query_data_non_binary(lsh_engine, query_vec, match_id, topK=None, multi_probe=False):\n",
        "    #old code that gives a distance proxy which is the number of buckets in which the item fell into same bucket as query vector\n",
        "    #if multi_probe:\n",
        "    #    matches = lsh_engine.FindMP(query_vec, 2)\n",
        "    #else:\n",
        "    #    matches = lsh_engine.Find(query_vec)\n",
        "    multi_probe_radius = 0\n",
        "    if multi_probe:\n",
        "        multi_probe_radius = 2\n",
        "    matches = lsh_engine.FindExact(query_vec, id_to_vec, multi_probe_radius)\n",
        "    if topK is None:\n",
        "        tuple_ids = [elem[0] for elem in matches]\n",
        "    else:\n",
        "        #Old code\n",
        "        #sorted_matches = sorted(matches, key=lambda x: x[1], reverse=True)\n",
        "        sorted_matches = sorted(matches, key=lambda x: x[1])\n",
        "        tuple_ids = [elem[0] for elem in matches[:topK]]\n",
        "    return match_id in tuple_ids, len(set(tuple_ids))\n",
        "\n",
        "def get_vector(data, dict_id_to_index, index, dimension=300):\n",
        "    vector_index = dict_id_to_index[index]\n",
        "    vector = data[vector_index]['vec']\n",
        "    vector = vector.reshape((dimension, 1))\n",
        "    return vector\n",
        "\n",
        "def compute_recall(lsh_engine, data, ground_truth_mapping, dict_id_to_index, topK=None, multi_probe=False):\n",
        "    total_duplicates = 0\n",
        "    found_duplicates = 0\n",
        "    total_comparisons = 0\n",
        "    for tuple_id1, tuple_id2 in ground_truth_mapping:\n",
        "        vector2 = get_vector(data, dict_id_to_index, tuple_id2)\n",
        "        result2, block_size = query_data_non_binary(lsh_engine, vector2, tuple_id1, topK, multi_probe)\n",
        "        total_duplicates = total_duplicates + 1\n",
        "        total_comparisons = total_comparisons + block_size\n",
        "        if result2 is True:\n",
        "            found_duplicates = found_duplicates + 1\n",
        "    return float(found_duplicates) / float(total_duplicates), total_comparisons\n",
        "\n",
        "\n",
        "def print_stats(torch_file_name, dataset1_size, dataset2_size):\n",
        "    data = torchfile.load(torch_file_name)\n",
        "    print(len(data), dataset1_size + dataset2_size)\n",
        "    print(data[0]['id'], data[dataset1_size-1]['id'])\n",
        "    print(data[dataset1_size]['id'], data[dataset1_size+dataset2_size-1]['id'], data[-1]['id'])\n",
        "\n",
        "\n",
        "def test_abt_buy(K, L):\n",
        "    lsh_engine = create_lsh(K, L)\n",
        "    ground_truth_mapping = get_csv_reader (\"abt_buy_perfectMapping.csv\")\n",
        "    data, dict_id_to_index  = get_torch_data_as_dicts(\"Abt-Buy.t7\")\n",
        "    dataset1_size = 1081\n",
        "    dataset2_size = 1092\n",
        "    index_data(lsh_engine, data, dataset1_size)\n",
        "    recall, block_size  = compute_recall(lsh_engine, data, ground_truth_mapping, dict_id_to_index)\n",
        "    return recall, block_size\n",
        "\n",
        "def test_amzn_google(K,L, topK=None, multi_probe=False):\n",
        "    lsh_engine = create_lsh(K, L)\n",
        "    ground_truth_mapping = get_csv_reader (\"Amzon_GoogleProducts_perfectMapping.csv\")\n",
        "    data, dict_id_to_index  = get_torch_data_as_dicts(\"Amazon-GoogleProducts.t7\")\n",
        "    dataset1_size = 1363\n",
        "    dataset2_size = 3226\n",
        "    index_data(lsh_engine, data, dataset1_size)\n",
        "    recall, block_size  = compute_recall(lsh_engine, data, ground_truth_mapping, dict_id_to_index, topK, multi_probe)\n",
        "    return recall, block_size\n",
        "\n",
        "def test_dblp_acm(K,L):\n",
        "    lsh_engine = create_lsh(K, L)\n",
        "    ground_truth_mapping = get_csv_reader (\"DBLP-ACM_perfectMapping.csv\")\n",
        "    data, dict_id_to_index  = get_torch_data_as_dicts(\"DBLP-ACM.t7\")\n",
        "    dataset1_size = 2616\n",
        "    dataset2_size = 2294\n",
        "    index_data(lsh_engine, data, dataset1_size)\n",
        "    recall, block_size  = compute_recall(lsh_engine, data, ground_truth_mapping, dict_id_to_index, multi_probe)\n",
        "    return recall, block_size\n",
        "\n",
        "def test_dblp_scholar(K,L, topK=None, multi_probe=False):\n",
        "    lsh_engine = create_lsh(K, L)\n",
        "    ground_truth_mapping = get_csv_reader (\"DBLP-Scholar_perfectMapping.csv\")\n",
        "    data, dict_id_to_index  = get_torch_data_as_dicts(\"DBLP-Scholar.t7\")\n",
        "    dataset1_size = 2616\n",
        "    dataset2_size = 64263\n",
        "    index_data(lsh_engine, data, dataset1_size)\n",
        "    recall, block_size  = compute_recall(lsh_engine, data, ground_truth_mapping, dict_id_to_index, topK)\n",
        "    return recall, block_size\n",
        "\n",
        "def test_dataset(test_fn_name, K, L, topK=None, multi_probe=False):\n",
        "    total_recall = 0\n",
        "    total_comparisons = 0\n",
        "    for random_seed in random_seeds:\n",
        "        #Lsh module uses numpy for random numbers\n",
        "        numpy.random.seed(random_seed)\n",
        "        recall, block_size = test_fn_name(K, L, topK, multi_probe)\n",
        "        #print K, L, random_seed, \"Recall: \", recall\n",
        "        total_recall = total_recall + recall\n",
        "        total_comparisons = total_comparisons + block_size\n",
        "    avg_recall = float(total_recall) / len(random_seeds)\n",
        "    avg_comparisons = float(total_comparisons) / len(random_seeds)\n",
        "    #print K, L, avg_recall\n",
        "    return avg_recall, avg_comparisons\n",
        "\n",
        "def test_dataset_diff_K_L(test_fn_name, op_file_name, dataset_name, range_K=10, range_L=10):\n",
        "    with open(op_file_name, 'a') as csv_file:\n",
        "        csv_writer = csv.writer(csv_file)\n",
        "        for K in range(1, range_K+1):\n",
        "            for L in range(1, range_L+1):\n",
        "                avg_recall, avg_comparisons = test_dataset(test_fn_name, K, L)\n",
        "                csv_writer.writerow([dataset_name, K, L, avg_recall, avg_comparisons])\n",
        "                csv_file.flush()\n",
        "\n",
        "def compute_K_L(n, P_1, P_2):\n",
        "    inv_P1 = 1.0 / P_1\n",
        "    inv_P2 = 1.0 / P_2\n",
        "\n",
        "    K = int( numpy.ceil( numpy.log(n) /  numpy.log(inv_P2)))\n",
        "\n",
        "    rho = numpy.log(inv_P1) / numpy.log(inv_P2)\n",
        "    L = int( numpy.ceil( numpy.power(n, rho) ))\n",
        "    return K, L\n",
        "\n",
        "\n",
        "def test_dataset_vary_P2(dataset_name, test_fn_name, n, P_1 = 0.95):\n",
        "    with open(\"vary_P2_fixed_P1.csv\", \"a\") as csv_file:\n",
        "        csv_writer = csv.writer(csv_file)\n",
        "\n",
        "        for P_2 in [0.5, 0.6, 0.7, 0.8, 0.9]:\n",
        "            K, L = compute_K_L(n, P_1, P_2)\n",
        "\n",
        "            avg_recall, avg_comparisons = test_dataset(test_fn_name, K, L) #run LSH for 10 runs\n",
        "            csv_writer.writerow([dataset_name, K, L, P_1, P_2, avg_recall, avg_comparisons])\n",
        "            csv_file.flush()\n",
        "\n",
        "\n",
        "def test_dataset_vary_P2_wrapper():\n",
        "    amazn_google_size = 1363\n",
        "    test_dataset_vary_P2(\"Amazon-Google\", test_amzn_google, amazn_google_size)\n",
        "    dblp_scholar_size = 2616\n",
        "    test_dataset_vary_P2(\"DBLP-Scholar\", test_dblp_scholar, dblp_scholar_size)\n",
        "\n",
        "\n",
        "def test_topK_vs_recall(test_fn_name, dataset_name, op_file_name, K, L, multi_probe=False):\n",
        "    with open(op_file_name, \"a\") as csv_file:\n",
        "        csv_writer = csv.writer(csv_file)\n",
        "        csv_writer.writerow([\"Dataset name\", \"K\", \"L\", \"TopK\", \"Avg Recall\", \"Avg Comparisons\", \"MultiProbe\"])\n",
        "        for topK in [10, 20, 30, 40, 50, 100]:\n",
        "            avg_recall, avg_comparisons = test_dataset(test_fn_name, K, L, topK, multi_probe)\n",
        "            csv_writer.writerow([dataset_name, K, L, topK, avg_recall, avg_comparisons, multi_probe])\n",
        "            csv_file.flush()\n",
        "\n",
        "def test_topK_vs_recall_wrapper():\n",
        "    K = 10\n",
        "    L = 1\n",
        "    #test_topK_vs_recall(test_amzn_google, \"Amzn-Google\", \"op_topK_vs_Recall.csv\", K, L, multi_probe=False)\n",
        "    #test_topK_vs_recall(test_dblp_scholar, \"DBLP-Scholar\", \"op_topK_vs_Recall.csv\", K, L, multi_probe=False)\n",
        "    test_topK_vs_recall(test_amzn_google, \"Amzn-Google\", \"op_topK_vs_Recall.csv\", K, L, multi_probe=True)\n",
        "    #test_topK_vs_recall(test_dblp_scholar, \"DBLP-Scholar\", \"op_topK_vs_Recall.csv\", K, L, multi_probe=True)\n",
        "\n",
        "def test_multi_probe_vs_recall_wrapper():\n",
        "    test_multi_probe_vs_recall(test_amzn_google, \"Amzn-Google\", \"op_multiprobe_vs_recall.csv\")\n",
        "    test_multi_probe_vs_recall(test_dblp_scholar, \"DBLP-Scholar\", \"op_multiprobe_vs_recall.csv\")\n",
        "\n",
        "\n",
        "#print time.ctime()\n",
        "#test_dataset_diff_K_L(test_abt_buy, \"op_all_K_vs_L.csv\", \"Abt_Buy\", 10, 10)\n",
        "#print time.ctime()\n",
        "#test_dataset_diff_K_L(test_amzn_google, \"op_all_K_vs_L.csv\", \"Amzn_Google\", 10, 10)\n",
        "#print time.ctime()\n",
        "#test_dataset_diff_K_L(test_dblp_acm, \"op_all_K_vs_L.csv\", \"DBLP_ACM\", 10, 10)\n",
        "#print time.ctime()\n",
        "#test_dataset_diff_K_L(test_dblp_scholar, \"op_all_K_vs_L.csv\", \"DBLP_Scholar\", 10, 10)\n",
        "#print time.ctime()\n",
        "\n",
        "#test_dataset_vary_P2_wrapper()\n",
        "\n",
        "#test_topK_vs_recall_wrapper()\n",
        "\n",
        "#test_multi_probe_vs_recall_wrapper()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFmxBSIUXfWh",
        "colab_type": "text"
      },
      "source": [
        "### Process dataset using absolute vector difference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nP5sDpq4Xnss",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#GiG.\n",
        "\n",
        "#This file mostly utility functions to process dataset files to the expected data format\n",
        "\n",
        "import csv\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import fasttext\n",
        "from scipy.spatial.distance import cosine\n",
        "import torch\n",
        "\n",
        "#This function splits an input file from a given path into three : train, validation and test\n",
        "# then output files train.csv, validation.csv, test.csv in the folder folder_path\n",
        "# conservatively, the split is done in a stratified manner by manually splitting data into duplicates and non duplicates\n",
        "# this is relevant when we test very small number of training data\n",
        "\n",
        "def split_dataset_by_ratio(folder_path, candset_ids_file_name, split_ratio=[0.3, 0.2, 0.5], label_field_name= 'gold', random_state=12345):\n",
        "    df = pd.read_csv(os.path.join(folder_path, candset_ids_file_name), encoding=\"utf-8\")\n",
        "    duplicates_df = df[df[label_field_name] == 1]\n",
        "    non_duplicates_df = df[df[label_field_name] == 0]\n",
        "\n",
        "    train_duplicates, validation_duplicates, test_duplicates = local_train_validate_test_split(duplicates_df, split_ratio, random_state)\n",
        "    train_non_duplicates, validation_non_duplicates, test_non_duplicates = local_train_validate_test_split(non_duplicates_df, split_ratio, random_state)\n",
        "\n",
        "    #The last sample is to shuffle the data so that duplicates and non_duplicates mix\n",
        "    train_df = pd.concat([train_duplicates, train_non_duplicates]).sample(frac=1)\n",
        "    validation_df = pd.concat([validation_duplicates, validation_non_duplicates]).sample(frac=1)\n",
        "    test_df = pd.concat([test_duplicates, test_non_duplicates]).sample(frac=1)\n",
        "\n",
        "    #verify_split(df, train_df, validation_df, test_df, split_ratio, label_field_name)\n",
        "    for partition_df, file_name in [(train_df, \"train.csv\"), (validation_df, \"validation.csv\"), (test_df, \"test.csv\")]:\n",
        "        partition_df.to_csv(os.path.join(folder_path, file_name), encoding=\"utf-8\", index=False)\n",
        "\n",
        "\n",
        "def local_train_validate_test_split(df, split_ratio=[0.3, 0.2, 0.5], random_state=12345):\n",
        "    np.random.seed(random_state)\n",
        "    random_shuffle = np.random.permutation(df.index)\n",
        "    num_tuples = len(df)\n",
        "\n",
        "    train_end = int(num_tuples * split_ratio[0])\n",
        "    validation_end = train_end + int(num_tuples * split_ratio[1])\n",
        "\n",
        "    train_df = df.ix[random_shuffle[:train_end]]\n",
        "    validation_df = df.ix[random_shuffle[train_end:validation_end]]\n",
        "    test_df = df.ix[random_shuffle[validation_end:]]\n",
        "\n",
        "    return train_df, validation_df, test_df\n",
        "\n",
        "#Trivial manual validation to check correctness of the split\n",
        "def verify_split(df, train_df, validation_df, test_df, split_ratio, label_field_name):\n",
        "    num_duplicates_df = len(df[df[label_field_name] == 1])\n",
        "    num_non_duplicates_df = len(df[df[label_field_name] == 0])\n",
        "\n",
        "    for index, partition_df in enumerate([train_df, validation_df, test_df]):\n",
        "        num_partition_duplicates = len(partition_df[partition_df[label_field_name] == 1])\n",
        "        num_partition_non_duplicates = len(partition_df[partition_df[label_field_name] == 0])\n",
        "        expected_duplicates, expected_non_duplicates = int(num_duplicates_df * split_ratio[index]), int(num_non_duplicates_df * split_ratio[index])\n",
        "        actual_duplicates, actual_non_duplicates = num_partition_duplicates, num_partition_non_duplicates\n",
        "        if actual_duplicates != expected_duplicates or actual_non_duplicates != expected_non_duplicates:\n",
        "            print(\"Mismatch :\", expected_duplicates, actual_duplicates, expected_non_duplicates, actual_non_duplicates)\n",
        "\n",
        "#Given a set of (ltable_id, rtable_id, gold) triples,\n",
        "# construct the distributional similarity vector and store it\n",
        "#First is the folder, second is the train|validation|test file, and the last two are the csv files of the left and right datasets\n",
        "#Output file name is obtained from input_file_name and is put in the same folder as folder_path\n",
        "# This is not very efficient - partially to avoid storing large intermediate matrices\n",
        "def dataset_to_matrix(folder_path, input_file_name, ltable_file_name, rtable_file_name):\n",
        "    ltable_df = pd.read_csv( os.path.join(folder_path, ltable_file_name), encoding=\"utf-8\")\n",
        "    rtable_df = pd.read_csv( os.path.join(folder_path, rtable_file_name), encoding=\"utf-8\")\n",
        "\n",
        "    candset_with_ids_df = pd.read_csv( os.path.join(folder_path, input_file_name), encoding=\"utf-8\" )\n",
        "\n",
        "    #Find common attributes of ltable and rtable\n",
        "    common_attributes = ltable_df.columns.intersection(rtable_df.columns)\n",
        "    #Only keep the common attributes\n",
        "    ltable_df = ltable_df[common_attributes]\n",
        "    rtable_df = rtable_df[common_attributes]\n",
        "\n",
        "    #Assumption id column is titled as \"id\"\n",
        "    #Remove this from nltk processing\n",
        "    common_attributes = list(common_attributes.drop(\"id\"))\n",
        "    ltable_df = ltable_df.fillna(\" \")\n",
        "    rtable_df = rtable_df.fillna(\" \")\n",
        "\n",
        "    #This is a list of punctuations to remove with empty string\n",
        "    replacement_list = get_replacement_list()\n",
        "\n",
        "    for attribute in common_attributes:\n",
        "        ltable_df[attribute] = ltable_df[attribute].replace(regex=replacement_list, value= \" \")\n",
        "        rtable_df[attribute] = rtable_df[attribute].replace(regex=replacement_list, value= \" \")\n",
        "\n",
        "    #Assumes key column name is id. Creates a dictionary where key is the id and the value is row index\n",
        "    id_to_idx_dict_ltable = dict(zip(ltable_df.id, ltable_df.index))\n",
        "    id_to_idx_dict_rtable = dict(zip(rtable_df.id, rtable_df.index))\n",
        "\n",
        "    #First m attributes are for cosine diff and next m for norm of abs diff\n",
        "    #The last one is for gold label\n",
        "    num_attributes = len(common_attributes)\n",
        "    dist_repr_similarity_matrix = np.zeros( (len(candset_with_ids_df), DR_DIMENSION * num_attributes + 1), dtype=np.float32)\n",
        "\n",
        "    #The following is an inefficient way to construct distributional similarity vectors\n",
        "    # but avoids the need to create huge distributional representations for input vectors\n",
        "    fasttext_model = load_fasttext_model()\n",
        "    for row_index, row in candset_with_ids_df.iterrows():\n",
        "        ltable_id, rtable_id, gold = row\n",
        "\n",
        "        #Get the index of the current (ltable_id, rtable_id, gold) being processed\n",
        "        ltable_index = id_to_idx_dict_ltable[ row['ltable_id'] ]\n",
        "        rtable_index = id_to_idx_dict_rtable[ row['rtable_id'] ]\n",
        "\n",
        "        #Get the corresponding rows\n",
        "        ltable_row = ltable_df.iloc[ltable_index]\n",
        "        rtable_row = rtable_df.iloc[rtable_index]\n",
        "\n",
        "        for col_index, attribute in enumerate(common_attributes):\n",
        "            abs_diff = compute_distance_abs_diff(fasttext_model, ltable_row[attribute], rtable_row[attribute])\n",
        "            start_pos = col_index * DR_DIMENSION\n",
        "            end_pos = (col_index+1) * DR_DIMENSION\n",
        "            dist_repr_similarity_matrix[row_index][start_pos:end_pos] = abs_diff\n",
        "            dist_repr_similarity_matrix[row_index][-1] = gold\n",
        "\n",
        "    np.save(os.path.join(folder_path, input_file_name), dist_repr_similarity_matrix)\n",
        "\n",
        "#This function takes two strings, converts to utf-8, computes their cosine and absolute error distance\n",
        "def compute_distance(fasttext_model, ltable_str, rtable_str):\n",
        "    if isinstance(ltable_str, basestring) == False:\n",
        "        ltable_str = unicode(ltable_str)\n",
        "    if isinstance(rtable_str, basestring) == False:\n",
        "        rtable_str = unicode(rtable_str)\n",
        "\n",
        "    lcol_dr = fasttext_model.get_sentence_vector(ltable_str)\n",
        "    rcol_dr = fasttext_model.get_sentence_vector(rtable_str)\n",
        "\n",
        "    #See the fillna command before for handling nulls\n",
        "    if ltable_str == rtable_str and ltable_str == \" \":\n",
        "        #If both empty return lowest distance\n",
        "        return 0.0, 0.0\n",
        "    if ltable_str == \" \" and rtable_str != \" \":\n",
        "        return 1.0, 1.0\n",
        "    if ltable_str != \" \" and rtable_str == \" \":\n",
        "        return 1.0, 1.0\n",
        "\n",
        "\n",
        "    cosine_dist = cosine(lcol_dr, rcol_dr)\n",
        "    normed_abs_dist = np.linalg.norm(np.abs(lcol_dr - rcol_dr))\n",
        "    return cosine_dist, normed_abs_dist\n",
        "\n",
        "#This function takes two strings, converts to utf-8, computes their cosine and absolute error distance\n",
        "def compute_distance_abs_diff(fasttext_model, ltable_str, rtable_str):\n",
        "    if isinstance(ltable_str, basestring) == False:\n",
        "        ltable_str = unicode(ltable_str)\n",
        "    if isinstance(rtable_str, basestring) == False:\n",
        "        rtable_str = unicode(rtable_str)\n",
        "\n",
        "    lcol_dr = fasttext_model.get_sentence_vector(ltable_str)\n",
        "    rcol_dr = fasttext_model.get_sentence_vector(rtable_str)\n",
        "\n",
        "    return np.abs(lcol_dr - rcol_dr)\n",
        "\n",
        "#This is a helper function to create distributional similarity matrix for all datasets\n",
        "# and calls the dist similarity computation for each of train, validation and test files\n",
        "def compute_dist_similarity_matrix_wrapper():\n",
        "    all_datasets = er_dataset_details.keys()\n",
        "    for dataset_name in all_datasets:\n",
        "\n",
        "        dataset = er_dataset_details[dataset_name]\n",
        "        folder = dataset[\"dataset_folder_path\"]\n",
        "        input_file = \"candset_ids_only.csv\"\n",
        "        ltable_file_name = dataset[\"ltable_file_name\"]\n",
        "        rtable_file_name = dataset[\"rtable_file_name\"]\n",
        "\n",
        "        #dataset_to_matrix(\"/Users/neo/Desktop/QCRI/DataCleaning/datasets/BenchmarkDatasets/Fodors_Zagat/\", \"candset_ids_only.csv\", \"fodors.csv\", \"zagats.csv\")\n",
        "        #dataset_to_matrix(folder, \"candset_ids_only.csv\", ltable_file, rtable_file)\n",
        "\n",
        "        print(\"Processing \", dataset_name, \" train.csv\")\n",
        "        dataset_to_matrix(folder, \"train.csv\", ltable_file_name, rtable_file_name)\n",
        "        print(\"Processing \", dataset_name, \" validation.csv\")\n",
        "        dataset_to_matrix(folder, \"validation.csv\", ltable_file_name, rtable_file_name)\n",
        "        print(\"Processing \", dataset_name, \" test.csv\")\n",
        "        dataset_to_matrix(folder, \"test.csv\", ltable_file_name, rtable_file_name)\n",
        "\n",
        "def convert_csv_to_features(dataset_name, input_file_name):\n",
        "    dataset = er_dataset_details[dataset_name]\n",
        "    folder_path = dataset[\"dataset_folder_path\"]\n",
        "    ltable_file_name = dataset[\"ltable_file_name\"]\n",
        "    rtable_file_name = dataset[\"rtable_file_name\"]\n",
        "\n",
        "    feature_file_name = input_file_name.replace(\".csv\", \"_abs_diff.npy\")\n",
        "\n",
        "    #Check if the npy file already exists\n",
        "    file_path = os.path.join(folder_path, feature_file_name)\n",
        "    if os.path.exists(file_path):\n",
        "        print(\"File {} already exists. Reusing it.\".format(feature_file_name))\n",
        "    else:\n",
        "        print(\"File {} does not exist. Creating and persisting it.\".format(feature_file_name))\n",
        "        dataset_to_matrix(folder_path, input_file_name, ltable_file_name, rtable_file_name, feature_file_name)\n",
        "    return np.load(file_path)\n",
        "\n",
        "def get_features_and_labels(dataset_name, file_name):\n",
        "    matrix = convert_csv_to_features(dataset_name, file_name)\n",
        "    features, labels = matrix[:, :-1], matrix[:, -1]\n",
        "\n",
        "    #Convert to torch format from numpy format\n",
        "    features, labels = torch.from_numpy(features), torch.from_numpy(labels).type(torch.LongTensor)\n",
        "    return features, labels\n",
        "\n",
        "\n",
        "#Function to efficiently remove common punctuation and stopwords\n",
        "def get_replacement_list():\n",
        "    replacement_list = [r'\\.', r\"\\'\", r'\\\"', r'\\(', '\\)', r'\\,', r'\\&', r'\\\\', r'\\/']\n",
        "    #for stopword in stopwords.words('english'):\n",
        "    #    replacement_list.append(r'\\b%s\\b' % stopword)\n",
        "    return replacement_list\n",
        "\n",
        "def load_fasttext_model():\n",
        "    return fastText.load_model(FASTTEXT_MODEL_PATH)\n",
        "\n",
        "def get_folder_to_persist_model(dataset_name):\n",
        "    dataset = er_dataset_details[dataset_name]\n",
        "    folder = dataset[\"dataset_folder_path\"]\n",
        "    return folder\n",
        "\n",
        "#if __name__ == \"__main__\":\n",
        "#    compute_dist_similarity_matrix_wrapper()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77NDYbXHXn77",
        "colab_type": "text"
      },
      "source": [
        "### Process dataset using similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDDE6BhyXv_u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#GiG.\n",
        "\n",
        "#This file mostly utility functions to process dataset files to the expected data format\n",
        "\n",
        "import csv\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "import fasttext\n",
        "from scipy.spatial.distance import cosine\n",
        "import torch\n",
        "\n",
        "#This function splits an input file from a given path into three : train, validation and test\n",
        "# then output files train.csv, validation.csv, test.csv in the folder folder_path\n",
        "# conservatively, the split is done in a stratified manner by manually splitting data into duplicates and non duplicates\n",
        "# this is relevant when we test very small number of training data\n",
        "\n",
        "def split_dataset_by_ratio(folder_path, candset_ids_file_name, split_ratio=[0.3, 0.2, 0.5], label_field_name= 'gold', random_state=12345, train_file_name=\"train.csv\", validation_file_name=\"validation.csv\", test_file_name=\"test.csv\"):\n",
        "    df = pd.read_csv(os.path.join(folder_path, candset_ids_file_name), encoding=\"utf-8\")\n",
        "    duplicates_df = df[df[label_field_name] == 1]\n",
        "    non_duplicates_df = df[df[label_field_name] == 0]\n",
        "\n",
        "    train_duplicates, validation_duplicates, test_duplicates = local_train_validate_test_split(duplicates_df, split_ratio, random_state)\n",
        "    train_non_duplicates, validation_non_duplicates, test_non_duplicates = local_train_validate_test_split(non_duplicates_df, split_ratio, random_state)\n",
        "\n",
        "    #The last sample is to shuffle the data so that duplicates and non_duplicates mix\n",
        "    train_df = pd.concat([train_duplicates, train_non_duplicates]).sample(frac=1)\n",
        "    validation_df = pd.concat([validation_duplicates, validation_non_duplicates]).sample(frac=1)\n",
        "    test_df = pd.concat([test_duplicates, test_non_duplicates]).sample(frac=1)\n",
        "\n",
        "    #verify_split(df, train_df, validation_df, test_df, split_ratio, label_field_name)\n",
        "    for partition_df, file_name in [(train_df, train_file_name), (validation_df, validation_file_name), (test_df, test_file_name)]:\n",
        "        partition_df.to_csv(os.path.join(folder_path, file_name), encoding=\"utf-8\", index=False)\n",
        "\n",
        "\n",
        "def local_train_validate_test_split(df, split_ratio=[0.3, 0.2, 0.5], random_state=12345):\n",
        "    np.random.seed(random_state)\n",
        "    random_shuffle = np.random.permutation(df.index)\n",
        "    num_tuples = len(df)\n",
        "\n",
        "    train_end = int(num_tuples * split_ratio[0])\n",
        "    validation_end = train_end + int(num_tuples * split_ratio[1])\n",
        "\n",
        "    train_df = df.ix[random_shuffle[:train_end]]\n",
        "    validation_df = df.ix[random_shuffle[train_end:validation_end]]\n",
        "    test_df = df.ix[random_shuffle[validation_end:]]\n",
        "\n",
        "    return train_df, validation_df, test_df\n",
        "\n",
        "#Trivial manual validation to check correctness of the split\n",
        "def verify_split(df, train_df, validation_df, test_df, split_ratio, label_field_name):\n",
        "    num_duplicates_df = len(df[df[label_field_name] == 1])\n",
        "    num_non_duplicates_df = len(df[df[label_field_name] == 0])\n",
        "\n",
        "    for index, partition_df in enumerate([train_df, validation_df, test_df]):\n",
        "        num_partition_duplicates = len(partition_df[partition_df[label_field_name] == 1])\n",
        "        num_partition_non_duplicates = len(partition_df[partition_df[label_field_name] == 0])\n",
        "        expected_duplicates, expected_non_duplicates = int(num_duplicates_df * split_ratio[index]), int(num_non_duplicates_df * split_ratio[index])\n",
        "        actual_duplicates, actual_non_duplicates = num_partition_duplicates, num_partition_non_duplicates\n",
        "        if actual_duplicates != expected_duplicates or actual_non_duplicates != expected_non_duplicates:\n",
        "            print(\"Mismatch :\", expected_duplicates, actual_duplicates, expected_non_duplicates, actual_non_duplicates)\n",
        "\n",
        "#Given a set of (ltable_id, rtable_id, gold) triples,\n",
        "# construct the distributional similarity vector and store it\n",
        "#First is the folder, second is the train|validation|test file, and the last two are the csv files of the left and right datasets\n",
        "#Output file name is obtained from input_file_name and is put in the same folder as folder_path\n",
        "# This is not very efficient - partially to avoid storing large intermediate matrices\n",
        "def dataset_to_matrix(folder_path, input_file_name, ltable_file_name, rtable_file_name):\n",
        "    ltable_df = pd.read_csv( os.path.join(folder_path, ltable_file_name), encoding=\"utf-8\")\n",
        "    rtable_df = pd.read_csv( os.path.join(folder_path, rtable_file_name), encoding=\"utf-8\")\n",
        "\n",
        "    candset_with_ids_df = pd.read_csv( os.path.join(folder_path, input_file_name), encoding=\"utf-8\" )\n",
        "\n",
        "    #Find common attributes of ltable and rtable\n",
        "    common_attributes = ltable_df.columns.intersection(rtable_df.columns)\n",
        "    #Only keep the common attributes\n",
        "    ltable_df = ltable_df[common_attributes]\n",
        "    rtable_df = rtable_df[common_attributes]\n",
        "\n",
        "    #Assumption id column is titled as \"id\"\n",
        "    #Remove this from nltk processing\n",
        "    common_attributes = list(common_attributes.drop(\"id\"))\n",
        "    ltable_df = ltable_df.fillna(\" \")\n",
        "    rtable_df = rtable_df.fillna(\" \")\n",
        "\n",
        "    #This is a list of punctuations to remove with empty string\n",
        "    replacement_list = get_replacement_list()\n",
        "\n",
        "    for attribute in common_attributes:\n",
        "        ltable_df[attribute] = ltable_df[attribute].replace(regex=replacement_list, value= \" \")\n",
        "        rtable_df[attribute] = rtable_df[attribute].replace(regex=replacement_list, value= \" \")\n",
        "\n",
        "    #Assumes key column name is id. Creates a dictionary where key is the id and the value is row index\n",
        "    id_to_idx_dict_ltable = dict(zip(ltable_df.id, ltable_df.index))\n",
        "    id_to_idx_dict_rtable = dict(zip(rtable_df.id, rtable_df.index))\n",
        "\n",
        "    #First m attributes are for cosine diff and next m for norm of abs diff\n",
        "    #The last one is for gold label\n",
        "    num_attributes = len(common_attributes)\n",
        "    dist_repr_similarity_matrix = np.zeros( (len(candset_with_ids_df), 2 * num_attributes + 1), dtype=np.float32)\n",
        "\n",
        "    #The following is an inefficient way to construct distributional similarity vectors\n",
        "    # but avoids the need to create huge distributional representations for input vectors\n",
        "    fasttext_model = load_fasttext_model()\n",
        "    for row_index, row in candset_with_ids_df.iterrows():\n",
        "        ltable_id, rtable_id, gold = row\n",
        "\n",
        "        #Get the index of the current (ltable_id, rtable_id, gold) being processed\n",
        "        ltable_index = id_to_idx_dict_ltable[ row['ltable_id'] ]\n",
        "        rtable_index = id_to_idx_dict_rtable[ row['rtable_id'] ]\n",
        "\n",
        "        #Get the corresponding rows\n",
        "        ltable_row = ltable_df.iloc[ltable_index]\n",
        "        rtable_row = rtable_df.iloc[rtable_index]\n",
        "\n",
        "        for col_index, attribute in enumerate(common_attributes):\n",
        "            cosine_dist, normed_abs_dist = compute_distance(fasttext_model, ltable_row[attribute], rtable_row[attribute])\n",
        "            dist_repr_similarity_matrix[row_index][col_index] = cosine_dist\n",
        "            dist_repr_similarity_matrix[row_index][num_attributes+col_index] = normed_abs_dist\n",
        "            dist_repr_similarity_matrix[row_index][-1] = gold\n",
        "\n",
        "    output_file_name = input_file_name.replace(\".csv\", \"\")\n",
        "    np.save(os.path.join(folder_path, output_file_name), dist_repr_similarity_matrix)\n",
        "\n",
        "#This function takes two strings, converts to utf-8, computes their cosine and absolute error distance\n",
        "def compute_distance(fasttext_model, ltable_str, rtable_str):\n",
        "    if isinstance(ltable_str, basestring) == False:\n",
        "        ltable_str = unicode(ltable_str)\n",
        "    if isinstance(rtable_str, basestring) == False:\n",
        "        rtable_str = unicode(rtable_str)\n",
        "\n",
        "    lcol_dr = fasttext_model.get_sentence_vector(ltable_str)\n",
        "    rcol_dr = fasttext_model.get_sentence_vector(rtable_str)\n",
        "\n",
        "    #See the fillna command before for handling nulls\n",
        "    if ltable_str == rtable_str and ltable_str == \" \":\n",
        "        #If both empty return lowest distance\n",
        "        return 0.0, 0.0\n",
        "    if ltable_str == \" \" and rtable_str != \" \":\n",
        "        return 1.0, 1.0\n",
        "    if ltable_str != \" \" and rtable_str == \" \":\n",
        "        return 1.0, 1.0\n",
        "\n",
        "\n",
        "    cosine_dist = cosine(lcol_dr, rcol_dr)\n",
        "    normed_abs_dist = np.linalg.norm(np.abs(lcol_dr - rcol_dr))\n",
        "    return cosine_dist, normed_abs_dist\n",
        "\n",
        "\n",
        "#This is a helper function to create distributional similarity matrix for all datasets\n",
        "# and calls the dist similarity computation for each of train, validation and test files\n",
        "def compute_dist_similarity_matrix_wrapper(dataset_name, train_file_name=\"train.csv\", validation_file_name=\"validation.csv\", test_file_name=\"test.csv\"):\n",
        "    dataset = er_dataset_details[dataset_name]\n",
        "    folder = dataset[\"dataset_folder_path\"]\n",
        "    input_file = \"candset_ids_only.csv\"\n",
        "    ltable_file_name = dataset[\"ltable_file_name\"]\n",
        "    rtable_file_name = dataset[\"rtable_file_name\"]\n",
        "\n",
        "    #dataset_to_matrix(\"/Users/neo/Desktop/QCRI/DataCleaning/datasets/BenchmarkDatasets/Fodors_Zagat/\", \"candset_ids_only.csv\", \"fodors.csv\", \"zagats.csv\")\n",
        "    #dataset_to_matrix(folder, \"candset_ids_only.csv\", ltable_file, rtable_file)\n",
        "\n",
        "    print(\"Processing \", dataset_name, train_file_name)\n",
        "    dataset_to_matrix(folder, train_file_name, ltable_file_name, rtable_file_name)\n",
        "    print(\"Processing \", dataset_name, validation_file_name)\n",
        "    dataset_to_matrix(folder, validation_file_name, ltable_file_name, rtable_file_name)\n",
        "    print(\"Processing \", dataset_name, test_file_name)\n",
        "    dataset_to_matrix(folder, test_file_name, ltable_file_name, rtable_file_name)\n",
        "\n",
        "def convert_csv_to_features(dataset_name, input_file_name):\n",
        "    dataset = er_dataset_details[dataset_name]\n",
        "    folder_path = dataset[\"dataset_folder_path\"]\n",
        "    ltable_file_name = dataset[\"ltable_file_name\"]\n",
        "    rtable_file_name = dataset[\"rtable_file_name\"]\n",
        "\n",
        "    feature_file_name = input_file_name.replace(\".csv\", \".npy\")\n",
        "\n",
        "    #Check if the npy file already exists\n",
        "    file_path = os.path.join(folder_path, feature_file_name)\n",
        "    if os.path.exists(file_path):\n",
        "        print(\"File {} already exists. Reusing it.\".format(feature_file_name))\n",
        "    else:\n",
        "        print(\"File {} does not exist. Creating and persisting it.\".format(feature_file_name))\n",
        "        dataset_to_matrix(folder_path, input_file_name, ltable_file_name, rtable_file_name)\n",
        "    return np.load(file_path)\n",
        "\n",
        "def get_features_and_labels(dataset_name, file_name):\n",
        "    matrix = convert_csv_to_features(dataset_name, file_name)\n",
        "    features, labels = matrix[:, :-1], matrix[:, -1]\n",
        "\n",
        "    #Convert to torch format from numpy format\n",
        "    features, labels = torch.from_numpy(features), torch.from_numpy(labels).type(torch.LongTensor)\n",
        "    return features, labels\n",
        "\n",
        "\n",
        "#Function to efficiently remove common punctuation and stopwords\n",
        "def get_replacement_list():\n",
        "    replacement_list = [r'\\.', r\"\\'\", r'\\\"', r'\\(', '\\)', r'\\,', r'\\&', r'\\\\', r'\\/']\n",
        "    #for stopword in stopwords.words('english'):\n",
        "    #    replacement_list.append(r'\\b%s\\b' % stopword)\n",
        "    return replacement_list\n",
        "\n",
        "def load_fasttext_model():\n",
        "    return fastText.load_model(FASTTEXT_MODEL_PATH)\n",
        "\n",
        "def get_folder_to_persist_model(dataset_name):\n",
        "    dataset = er_dataset_details[dataset_name]\n",
        "    folder = dataset[\"dataset_folder_path\"]\n",
        "    return folder\n",
        "\n",
        "#Sample code to create training, validation and testing files\n",
        "#Change the parameters below as necessary\n",
        "def temp_sample_split_code():\n",
        "    dataset_name = \"DBLP_Scholar\"\n",
        "    folder_path = er_dataset_details[dataset_name][\"dataset_folder_path\"]\n",
        "    candset_ids_file_name = \"candset_ids_only.csv\"\n",
        "    split_dataset_by_ratio(folder_path, candset_ids_file_name, split_ratio=[0.3, 0.2, 0.5], label_field_name= 'gold', random_state=12345, train_file_name=\"train.csv\",           validation_file_name=\"validation.csv\", test_file_name=\"test.csv\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBUPd0WPXwVT",
        "colab_type": "text"
      },
      "source": [
        "### DeepER using similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZyLRpl7X64A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "e29fd821-5552-4a5b-a201-8cc2cb733e96"
      },
      "source": [
        "#GiG.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as Data\n",
        "import torch.optim as optim\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#DL Specific configs\n",
        "BATCH_SIZE = 16\n",
        "MAX_EPOCHS = 32\n",
        "LEARNING_RATE = 0.001\n",
        "BETAS = (0.9, 0.99)\n",
        "EPSILON = 1e-9\n",
        "RANDOM_STATE = 12345\n",
        "HIDDEN_X = 2\n",
        "MODEL_FILE_NAME = \"best_validation_model_params.torch\"\n",
        "\n",
        "def get_deeper_lite_model_sim(num_attributes):\n",
        "\n",
        "    #If there are K input attributes, Deeper Lite  has 2K features : 1 each for cosine distance and normed abs distance\n",
        "    #Hidden_X is a multiplicative factor controlling the size of hidden layer.\n",
        "    deeper_lite_model = nn.Sequential(\n",
        "        nn.Linear(int(2 * num_attributes), int(HIDDEN_X * num_attributes)),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(int(HIDDEN_X * num_attributes), int(HIDDEN_X * num_attributes)),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(int(HIDDEN_X * num_attributes), int(HIDDEN_X * num_attributes)),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(int(HIDDEN_X * num_attributes), 2),\n",
        "    )\n",
        "\n",
        "    return deeper_lite_model\n",
        "\n",
        "#Assumes that the train and validation files are in the same folder as dataset_name\n",
        "def train(dataset_name, train_file_name, validation_file_name, model_fn):\n",
        "    train_features, train_labels = get_features_and_labels(dataset_name, train_file_name)\n",
        "    validation_features, validation_labels = get_features_and_labels(dataset_name, validation_file_name)\n",
        "\n",
        "    #Hack: Assumes that for deeper lite num_features = 2 * num_attributes\n",
        "    num_attributes = int(train_features.shape[1] / 2)\n",
        "    model = model_fn(num_attributes)\n",
        "\n",
        "    train_dataset = Data.TensorDataset(train_features, train_labels)\n",
        "    #Allows us to read the dataset in batches\n",
        "    training_loader = Data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, betas=BETAS, eps=EPSILON)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "    #For reproducibility\n",
        "    random.seed(RANDOM_STATE)\n",
        "    np.random.seed(RANDOM_STATE)\n",
        "    torch.manual_seed(RANDOM_STATE)\n",
        "\n",
        "    best_validation_f1_score = 0.0\n",
        "    best_model_so_far = None\n",
        "    model_file_name_path = os.path.join(get_folder_to_persist_model(dataset_name) , MODEL_FILE_NAME)\n",
        "\n",
        "    for epoch in range(MAX_EPOCHS):\n",
        "        for batch_idx, (train_features, train_labels) in enumerate(training_loader):\n",
        "            optimizer.zero_grad()\n",
        "            output = model(train_features)\n",
        "            loss = criterion(output, train_labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        training_f1_score = compute_scores(output, train_labels)\n",
        "        with torch.no_grad():\n",
        "            validation_output = model(validation_features)\n",
        "            validation_f1_score = compute_scores(validation_output, validation_labels)\n",
        "            if validation_f1_score > best_validation_f1_score:\n",
        "                best_model_so_far = model.state_dict()\n",
        "                best_validation_f1_score = validation_f1_score\n",
        "\n",
        "    torch.save(best_model_so_far, model_file_name_path)\n",
        "    print(\"Curr Val F1, Best Val F1 \", validation_f1_score, best_validation_f1_score)\n",
        "    return best_model_so_far\n",
        "\n",
        "def test(dataset_name, test_file_name, test_output_file_name, model_fn):\n",
        "    test_features, test_labels = get_features_and_labels(dataset_name, test_file_name)\n",
        "    #Hack: Assumes that for deeper lite num_features = 2 * num_attributes\n",
        "    num_attributes = test_features.shape[1] / 2\n",
        "    model = model_fn(num_attributes)\n",
        "\n",
        "    folder_path = get_folder_to_persist_model(dataset_name)\n",
        "    model_file_name_path = os.path.join( folder_path, MODEL_FILE_NAME)\n",
        "    model.load_state_dict(torch.load(model_file_name_path))\n",
        "    model.eval()\n",
        "\n",
        "    predictions = model(test_features)\n",
        "    #Uncomment the following lines to get the score\n",
        "    #testing_f1_score = compute_scores(predictions, test_labels)\n",
        "    #print \"Testing F1 \", testing_f1_score\n",
        "\n",
        "    prediction_as_numpy = torch.max(predictions, 1)[1].data.numpy()\n",
        "\n",
        "    #Store output\n",
        "    test_df = pd.read_csv( os.path.join(folder_path, test_file_name), encoding=\"utf-8\" )\n",
        "    test_df[\"gold\"] = prediction_as_numpy\n",
        "    test_df.to_csv(os.path.join(folder_path, test_output_file_name), encoding=\"utf8\", index=False)\n",
        "\n",
        "def compute_scores(predicted, actual):\n",
        "    #Convert from cross entropy output to actual 0/1 predictions\n",
        "    predicted = torch.max(predicted, 1)[1].data\n",
        "\n",
        "    #Convert to numpy format\n",
        "    predicted_numpy = predicted.numpy()\n",
        "    actual_numpy = actual.numpy()\n",
        "\n",
        "    #Print performance measures\n",
        "    return f1_score(actual_numpy, predicted_numpy)\n",
        "\n",
        "#if __name__ == \"__main__\":\n",
        "train(\"Fodors_Zagat\", \"train.csv\", \"validation.csv\", get_deeper_lite_model_sim)\n",
        "test(\"Fodors_Zagat\", \"test.csv\", \"test_predictions.csv\", get_deeper_lite_model_sim)\n",
        "\n",
        "    #train(\"Cora\", \"train.csv\", \"validation.csv\", get_deeper_lite_model_sim)\n",
        "    #test(\"Cora\", \"test.csv\", \"test_predictions.csv\", get_deeper_lite_model_sim)\n",
        "\n",
        "    #train(\"DBLP_ACM\", \"train.csv\", \"validation.csv\", get_deeper_lite_model_sim)\n",
        "    #test(\"DBLP_ACM\", \"test.csv\", \"test_predictions.csv\", get_deeper_lite_model_sim)\n",
        "\n",
        "    #train(\"DBLP_Scholar\", \"train.csv\", \"validation.csv\", get_deeper_lite_model_sim)\n",
        "    #test(\"DBLP_Scholar\", \"test.csv\", \"test_predictions.csv\", get_deeper_lite_model_sim)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File train.npy already exists. Reusing it.\n",
            "File validation.npy already exists. Reusing it.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true samples.\n",
            "  'recall', 'true', average, warn_for)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Curr Val F1, Best Val F1  1.0 1.0\n",
            "File test.npy already exists. Reusing it.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyW4qmuIX7cp",
        "colab_type": "text"
      },
      "source": [
        "### DeepER using absolute vector difference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0PDqHOxYGXO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#GiG.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as Data\n",
        "import torch.optim as optim\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import process_dataset_sim as process_dataset\n",
        "\n",
        "#DL Specific configs\n",
        "BATCH_SIZE = 16\n",
        "MAX_EPOCHS = 32\n",
        "LEARNING_RATE = 0.001\n",
        "BETAS = (0.9, 0.99)\n",
        "EPSILON = 1e-9\n",
        "RANDOM_STATE = 12345\n",
        "HIDDEN_X = 2\n",
        "MODEL_FILE_NAME = \"best_validation_model_params.torch\"\n",
        "\n",
        "def get_deeper_lite_model_sim(num_attributes):\n",
        "\n",
        "    #If there are K input attributes, Deeper Lite  has 2K features : 1 each for cosine distance and normed abs distance\n",
        "    #Hidden_X is a multiplicative factor controlling the size of hidden layer.\n",
        "    deeper_lite_model = nn.Sequential(\n",
        "        nn.Linear(2 * num_attributes, HIDDEN_X * num_attributes),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(HIDDEN_X * num_attributes, HIDDEN_X * num_attributes),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(HIDDEN_X * num_attributes, HIDDEN_X * num_attributes),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(HIDDEN_X * num_attributes, 2),\n",
        "    )\n",
        "\n",
        "    return deeper_lite_model\n",
        "\n",
        "#Assumes that the train and validation files are in the same folder as dataset_name\n",
        "def train(dataset_name, train_file_name, validation_file_name, model_fn):\n",
        "    train_features, train_labels = process_dataset.get_features_and_labels(dataset_name, train_file_name)\n",
        "    validation_features, validation_labels = process_dataset.get_features_and_labels(dataset_name, validation_file_name)\n",
        "\n",
        "    #Hack: Assumes that for deeper lite num_features = 2 * num_attributes\n",
        "    num_attributes = train_features.shape[1] / 2\n",
        "    model = model_fn(num_attributes)\n",
        "\n",
        "    train_dataset = Data.TensorDataset(train_features, train_labels)\n",
        "    #Allows us to read the dataset in batches\n",
        "    training_loader = Data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, betas=BETAS, eps=EPSILON)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "    #For reproducibility\n",
        "    random.seed(RANDOM_STATE)\n",
        "    np.random.seed(RANDOM_STATE)\n",
        "    torch.manual_seed(RANDOM_STATE)\n",
        "\n",
        "    best_validation_f1_score = 0.0\n",
        "    best_model_so_far = None\n",
        "    model_file_name_path = os.path.join( process_dataset.get_folder_to_persist_model(dataset_name) , MODEL_FILE_NAME)\n",
        "\n",
        "    for epoch in range(MAX_EPOCHS):\n",
        "        for batch_idx, (train_features, train_labels) in enumerate(training_loader):\n",
        "            optimizer.zero_grad()\n",
        "            output = model(train_features)\n",
        "            loss = criterion(output, train_labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        training_f1_score = compute_scores(output, train_labels)\n",
        "        with torch.no_grad():\n",
        "            validation_output = model(validation_features)\n",
        "            validation_f1_score = compute_scores(validation_output, validation_labels)\n",
        "            if validation_f1_score > best_validation_f1_score:\n",
        "                best_model_so_far = model.state_dict()\n",
        "                best_validation_f1_score = validation_f1_score\n",
        "\n",
        "    torch.save(best_model_so_far, model_file_name_path)\n",
        "    print \"Curr Val F1, Best Val F1 \", validation_f1_score, best_validation_f1_score\n",
        "    return best_model_so_far\n",
        "\n",
        "def test(dataset_name, test_file_name, test_output_file_name, model_fn):\n",
        "    test_features, test_labels = process_dataset.get_features_and_labels(dataset_name, test_file_name)\n",
        "    #Hack: Assumes that for deeper lite num_features = 2 * num_attributes\n",
        "    num_attributes = test_features.shape[1] / 2\n",
        "    model = model_fn(num_attributes)\n",
        "\n",
        "    folder_path = process_dataset.get_folder_to_persist_model(dataset_name)\n",
        "    model_file_name_path = os.path.join( folder_path, MODEL_FILE_NAME)\n",
        "    model.load_state_dict(torch.load(model_file_name_path))\n",
        "    model.eval()\n",
        "\n",
        "    predictions = model(test_features)\n",
        "    #Uncomment the following lines to get the score\n",
        "    #testing_f1_score = compute_scores(predictions, test_labels)\n",
        "    #print \"Testing F1 \", testing_f1_score\n",
        "\n",
        "    prediction_as_numpy = torch.max(predictions, 1)[1].data.numpy()\n",
        "\n",
        "    #Store output\n",
        "    test_df = pd.read_csv( os.path.join(folder_path, test_file_name), encoding=\"utf-8\" )\n",
        "    test_df[\"gold\"] = prediction_as_numpy\n",
        "    test_df.to_csv(os.path.join(folder_path, test_output_file_name), encoding=\"utf8\", index=False)\n",
        "\n",
        "def compute_scores(predicted, actual):\n",
        "    #Convert from cross entropy output to actual 0/1 predictions\n",
        "    predicted = torch.max(predicted, 1)[1].data\n",
        "\n",
        "    #Convert to numpy format\n",
        "    predicted_numpy = predicted.numpy()\n",
        "    actual_numpy = actual.numpy()\n",
        "\n",
        "    #Print performance measures\n",
        "    return f1_score(actual_numpy, predicted_numpy)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train(\"Fodors_Zagat\", \"train.csv\", \"validation.csv\", get_deeper_lite_model_sim)\n",
        "    test(\"Fodors_Zagat\", \"test.csv\", \"test_predictions.csv\", get_deeper_lite_model_sim)\n",
        "\n",
        "    train(\"Cora\", \"train.csv\", \"validation.csv\", get_deeper_lite_model_sim)\n",
        "    test(\"Cora\", \"test.csv\", \"test_predictions.csv\", get_deeper_lite_model_sim)\n",
        "\n",
        "    train(\"DBLP_ACM\", \"train.csv\", \"validation.csv\", get_deeper_lite_model_sim)\n",
        "    test(\"DBLP_ACM\", \"test.csv\", \"test_predictions.csv\", get_deeper_lite_model_sim)\n",
        "\n",
        "    train(\"DBLP_Scholar\", \"train.csv\", \"validation.csv\", get_deeper_lite_model_sim)\n",
        "    test(\"DBLP_Scholar\", \"test.csv\", \"test_predictions.csv\", get_deeper_lite_model_sim)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}